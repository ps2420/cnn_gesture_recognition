{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc.pilutil import imread, imresize\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "from numpy import nanmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scipy==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/datasets/Project_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(data_dir + '/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open(data_dir + '/val.csv').readlines())\n",
    "batch_size = 40 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # No. of frames images\n",
    "#y = 120 # Width of the image\n",
    "#z = 120 # height\n",
    "\n",
    "# parameterize the width and height to test with different sizes\n",
    "def generator(source_path, folder_list, batch_size, width=120, height=120):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size# calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size, x, width, height, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    resized_image = imresize(image,(width, height)) ##default resample=1 or 'P' which indicates PIL.Image.NEAREST\n",
    "                    resized_image = resized_image/255\n",
    "                    \n",
    "                \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size * num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x, width, height, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized_image = imresize(image,(width, height)) ##default resample=1 or 'P' which indicates PIL.Image.NEAREST\n",
    "                    resized_image = resized_image/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (resized_image[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (resized_image[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (resized_image[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 15\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = data_dir + '/train'\n",
    "val_path = data_dir + '/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 15 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and define model-checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries to train model\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "#from keras import optimizers #Code provided by upgrad does not work\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# Note change the parameter to False, else disk would run out of space.\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model architectures to reuse later with different epoc, batch sizes and train model with different image sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_architecture:\n",
    "    # initialize default parameter\n",
    "    num_epochs = 25\n",
    "    batch_size = 40\n",
    "    image_width_height = 120\n",
    "    \n",
    "    #defining constructor  \n",
    "    def __init__(self, num_epochs = 15, batch_size = 40, image_width_height = 120):  \n",
    "        self.num_epochs = num_epochs  \n",
    "        self.batch_size = batch_size \n",
    "        self.image_width_height = image_width_height\n",
    "        \n",
    "    def define_architecture_1(self):\n",
    "        model_arch_1 = Sequential()       \n",
    "        model_arch_1.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(30, self.image_width_height, self.image_width_height, 3),padding='same'))\n",
    "        model_arch_1.add(BatchNormalization())\n",
    "        model_arch_1.add(Activation('relu'))\n",
    "\n",
    "        model_arch_1.add(Conv3D(16, (3, 3, 3), activation='relu', padding='same')) \n",
    "        model_arch_1.add(BatchNormalization())\n",
    "        model_arch_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model_arch_1.add(Conv3D(32, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_1.add(BatchNormalization())\n",
    "        model_arch_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model_arch_1.add(Conv3D(64, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_1.add(BatchNormalization())\n",
    "        model_arch_1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model_arch_1.add(Conv3D(128, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_1.add(BatchNormalization())\n",
    "        model_arch_1.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "        # Flatten layer \n",
    "        model_arch_1.add(Flatten())\n",
    "        model_arch_1.add(Dense(1000, activation='relu'))\n",
    "        model_arch_1.add(Dropout(0.5))\n",
    "\n",
    "        model_arch_1.add(Dense(500, activation='relu'))\n",
    "        model_arch_1.add(Dropout(0.5))\n",
    "\n",
    "        #Softmax layer\n",
    "        model_arch_1.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        optimiser = tf.keras.optimizers.Adam() #write your optimizer\n",
    "        model_arch_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        print (model_arch_1.summary())\n",
    "\n",
    "        self.model_arch = model_arch_1\n",
    "        \n",
    "    # different variant of architecture 1, with dropouts at more dense layer   \n",
    "    def define_architecture_2(self):\n",
    "        model_arch_2 = Sequential()       \n",
    "        model_arch_2.add(Conv3D(8,kernel_size=(3,3,3),input_shape=(30, self.image_width_height, self.image_width_height, 3),padding='same'))\n",
    "        model_arch_2.add(BatchNormalization())\n",
    "        model_arch_2.add(Activation('relu'))\n",
    "\n",
    "        model_arch_2.add(Conv3D(16, (3, 3, 3), activation='relu', padding='same')) \n",
    "        model_arch_2.add(BatchNormalization())\n",
    "        model_arch_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model_arch_2.add(Conv3D(32, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_2.add(BatchNormalization())\n",
    "        model_arch_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model_arch_2.add(Dropout(0.25))\n",
    "\n",
    "        model_arch_2.add(Conv3D(64, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_2.add(BatchNormalization())\n",
    "        model_arch_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model_arch_2.add(Dropout(0.25))\n",
    "\n",
    "        model_arch_2.add(Conv3D(128, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_2.add(BatchNormalization())\n",
    "        model_arch_2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model_arch_2.add(Dropout(0.25))\n",
    "\n",
    "        # Flatten layer \n",
    "        model_arch_2.add(Flatten())\n",
    "        model_arch_2.add(Dense(1000, activation='relu'))\n",
    "        model_arch_2.add(Dropout(0.5))\n",
    "\n",
    "        model_arch_2.add(Dense(500, activation='relu'))\n",
    "        model_arch_2.add(Dropout(0.5))\n",
    "\n",
    "        #Softmax layer\n",
    "        model_arch_2.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        optimiser = tf.keras.optimizers.Adam() #write your optimizer\n",
    "        model_arch_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        print (model_arch_2.summary())\n",
    "\n",
    "        self.model_arch = model_arch_2\n",
    "    \n",
    "    #architecture with different layer and drop-out at dense layer with 0.5 & 0.25\n",
    "    def define_architecture_3(self):\n",
    "        model_arch_3 = Sequential()       \n",
    "        model_arch_3.add(Conv3D(16,kernel_size=(3,3,3),input_shape=(30, self.image_width_height, self.image_width_height, 3),padding='same'))\n",
    "        model_arch_3.add(BatchNormalization())\n",
    "        model_arch_3.add(Activation('relu'))\n",
    "\n",
    "        model_arch_3.add(Conv3D(32, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_3.add(BatchNormalization())\n",
    "        model_arch_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model_arch_3.add(Conv3D(64, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_3.add(BatchNormalization())\n",
    "        model_arch_3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        model_arch_3.add(Conv3D(128, (2, 2, 2), activation='relu', padding='same')) \n",
    "        model_arch_3.add(BatchNormalization())\n",
    "        model_arch_3.add(MaxPooling3D(pool_size=(2, 2, 2)))      \n",
    "\n",
    "        # Flatten layer \n",
    "        model_arch_3.add(Flatten())\n",
    "        model_arch_3.add(Dense(512, activation='relu'))\n",
    "        model_arch_3.add(Dropout(0.5))\n",
    "\n",
    "        model_arch_3.add(Dense(256, activation='relu'))\n",
    "        model_arch_3.add(Dropout(0.25))\n",
    "\n",
    "        #Softmax layer\n",
    "        model_arch_3.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        optimiser = tf.keras.optimizers.Adam() #write your optimizer\n",
    "        model_arch_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        print (model_arch_3.summary())\n",
    "\n",
    "        self.model_arch = model_arch_3\n",
    "\n",
    "    \n",
    "    def cal_validation_steps_per_epoch(self):\n",
    "        if (num_train_sequences%self.batch_size) == 0:\n",
    "            steps_per_epoch = int(num_train_sequences/self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (num_train_sequences//self.batch_size) + 1\n",
    "\n",
    "        if (num_val_sequences%batch_size) == 0:\n",
    "            validation_steps = int(num_val_sequences/self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (num_val_sequences//self.batch_size) + 1\n",
    "\n",
    "        return steps_per_epoch, validation_steps\n",
    "\n",
    "    def plot(self, history):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "        axes[0].plot(history.history['loss'])   \n",
    "        axes[0].plot(history.history['val_loss'])\n",
    "        axes[0].grid()\n",
    "        axes[0].legend(['loss','val_loss'])\n",
    "        axes[1].plot(history.history['categorical_accuracy'])   \n",
    "        axes[1].plot(history.history['val_categorical_accuracy'])\n",
    "        axes[1].grid()\n",
    "        axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n",
    "    \n",
    "    def execute(self):\n",
    "        # Now that you have defined the model in define_architecture_1, the next step is to `compile` the model. \n",
    "        # When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "        train_generator = generator(train_path, train_doc, self.batch_size, self.image_width_height, self.image_width_height)\n",
    "        val_generator = generator(val_path, val_doc, self.batch_size, self.image_width_height, self.image_width_height)\n",
    "\n",
    "        #The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make.\n",
    "        steps_per_epoch, validation_steps = self.cal_validation_steps_per_epoch()\n",
    "\n",
    "        history = self.model_arch.fit_generator(train_generator, steps_per_epoch = steps_per_epoch, epochs = self.num_epochs, verbose=1, \n",
    "                        callbacks = callbacks_list, validation_data = val_generator, \n",
    "                        validation_steps = validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        self.plot(history)\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-1 input summary\n",
    "- input image size(120x120)\n",
    "- epochs: 15\n",
    "- batch size: 40\n",
    "    \n",
    "Run the model with few epochs to see if architecutre is working and validation loss is continuously decreasing. In initial few runs i.e. less number of epochs model should overfit i.e. training accuracy should be high and validation accuracy should be less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_26 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 30, 120, 120, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 30, 120, 120, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 30, 120, 120, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_16 (MaxPoolin  (None, 15, 60, 60, 16)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_28 (Conv3D)          (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 15, 60, 60, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPoolin  (None, 7, 30, 30, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 7, 30, 30, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 3, 15, 15, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 3, 15, 15, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 1, 7, 7, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 6.6549 - categorical_accuracy: 0.3273Source path =  /home/datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.83905 to 1.74027, saving model to model_init_2022-05-0714_40_18.671989/model-00001-6.65486-0.32730-1.74027-0.19000.h5\n",
      "17/17 [==============================] - 85s 5s/step - loss: 6.6549 - categorical_accuracy: 0.3273 - val_loss: 1.7403 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1179 - categorical_accuracy: 0.5371\n",
      "Epoch 00002: val_loss did not improve from 1.74027\n",
      "17/17 [==============================] - 51s 3s/step - loss: 2.1179 - categorical_accuracy: 0.5371 - val_loss: 2.7558 - val_categorical_accuracy: 0.2167 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4511 - categorical_accuracy: 0.6049\n",
      "Epoch 00003: val_loss did not improve from 1.74027\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.4511 - categorical_accuracy: 0.6049 - val_loss: 4.4899 - val_categorical_accuracy: 0.1000 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0935 - categorical_accuracy: 0.6749\n",
      "Epoch 00004: val_loss did not improve from 1.74027\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.0935 - categorical_accuracy: 0.6749 - val_loss: 6.8609 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-04\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9094 - categorical_accuracy: 0.7188\n",
      "Epoch 00005: val_loss did not improve from 1.74027\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.9094 - categorical_accuracy: 0.7188 - val_loss: 10.2555 - val_categorical_accuracy: 0.1333 - lr: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f85b8049670>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_1 = model_architecture(15, 40, 120)\n",
    "model_1.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_1.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-1 output summary\n",
    "- Training accuracy: 86.16\n",
    "- Validation accuracy: 15.00\n",
    "\n",
    "##### In initial run we see model overfits with less number of epochs. Our theory holds ture and it will be too early to say our model architecture needs change.\n",
    "Validation loss did not improve and fairly averaged around 6.9410. We should try to train with more number of epochs and various sizes and observe the output.\n",
    "We should try to experiment with more number of epochs and with different batch size for ex: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-2 input summary (Image-size: 120x120)\n",
    "- input image size(120x120)\n",
    "- epochs: 25\n",
    "- batch size: 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_5 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 30, 120, 120, 8)  32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 30, 120, 120, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 30, 120, 120, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 15, 60, 60, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 15, 60, 60, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 7, 30, 30, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 7, 30, 30, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 3, 15, 15, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 3, 15, 15, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 7, 7, 128)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 7.6836 - categorical_accuracy: 0.3243Source path =  /home/datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 79s 5s/step - loss: 7.6836 - categorical_accuracy: 0.3243 - val_loss: 1.7373 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.2913 - categorical_accuracy: 0.4092\n",
      "Epoch 00002: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 48s 3s/step - loss: 2.2913 - categorical_accuracy: 0.4092 - val_loss: 1.6099 - val_categorical_accuracy: 0.2333 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7001 - categorical_accuracy: 0.4550\n",
      "Epoch 00003: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.7001 - categorical_accuracy: 0.4550 - val_loss: 5.0531 - val_categorical_accuracy: 0.1833 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5181 - categorical_accuracy: 0.5108\n",
      "Epoch 00004: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 41s 3s/step - loss: 1.5181 - categorical_accuracy: 0.5108 - val_loss: 5.8344 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3038 - categorical_accuracy: 0.5559\n",
      "Epoch 00005: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 39s 2s/step - loss: 1.3038 - categorical_accuracy: 0.5559 - val_loss: 8.4491 - val_categorical_accuracy: 0.2167 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9402 - categorical_accuracy: 0.6540\n",
      "Epoch 00006: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.9402 - categorical_accuracy: 0.6540 - val_loss: 10.5502 - val_categorical_accuracy: 0.1167 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8313 - categorical_accuracy: 0.6782\n",
      "Epoch 00007: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.8313 - categorical_accuracy: 0.6782 - val_loss: 9.2902 - val_categorical_accuracy: 0.2667 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5751 - categorical_accuracy: 0.7612\n",
      "Epoch 00008: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.5751 - categorical_accuracy: 0.7612 - val_loss: 8.5239 - val_categorical_accuracy: 0.2667 - lr: 2.5000e-04\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6112 - categorical_accuracy: 0.7820\n",
      "Epoch 00009: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.6112 - categorical_accuracy: 0.7820 - val_loss: 10.7896 - val_categorical_accuracy: 0.1833 - lr: 1.2500e-04\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5708 - categorical_accuracy: 0.7924\n",
      "Epoch 00010: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "17/17 [==============================] - 36s 2s/step - loss: 0.5708 - categorical_accuracy: 0.7924 - val_loss: 12.1193 - val_categorical_accuracy: 0.2000 - lr: 1.2500e-04\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4563 - categorical_accuracy: 0.8028\n",
      "Epoch 00011: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4563 - categorical_accuracy: 0.8028 - val_loss: 11.3598 - val_categorical_accuracy: 0.1500 - lr: 6.2500e-05\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5330 - categorical_accuracy: 0.7993\n",
      "Epoch 00012: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.5330 - categorical_accuracy: 0.7993 - val_loss: 10.3823 - val_categorical_accuracy: 0.2000 - lr: 6.2500e-05\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5270 - categorical_accuracy: 0.7993\n",
      "Epoch 00013: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.5270 - categorical_accuracy: 0.7993 - val_loss: 11.2869 - val_categorical_accuracy: 0.2167 - lr: 3.1250e-05\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4631 - categorical_accuracy: 0.8235\n",
      "Epoch 00014: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.4631 - categorical_accuracy: 0.8235 - val_loss: 8.2108 - val_categorical_accuracy: 0.2667 - lr: 3.1250e-05\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3770 - categorical_accuracy: 0.8685\n",
      "Epoch 00015: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3770 - categorical_accuracy: 0.8685 - val_loss: 9.5008 - val_categorical_accuracy: 0.2000 - lr: 1.5625e-05\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4326 - categorical_accuracy: 0.8235\n",
      "Epoch 00016: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4326 - categorical_accuracy: 0.8235 - val_loss: 10.3267 - val_categorical_accuracy: 0.1500 - lr: 1.5625e-05\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4491 - categorical_accuracy: 0.8478\n",
      "Epoch 00017: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4491 - categorical_accuracy: 0.8478 - val_loss: 8.1457 - val_categorical_accuracy: 0.2500 - lr: 7.8125e-06\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4452 - categorical_accuracy: 0.8478\n",
      "Epoch 00018: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "17/17 [==============================] - 40s 3s/step - loss: 0.4452 - categorical_accuracy: 0.8478 - val_loss: 8.1408 - val_categorical_accuracy: 0.1667 - lr: 7.8125e-06\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4042 - categorical_accuracy: 0.8581\n",
      "Epoch 00019: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.4042 - categorical_accuracy: 0.8581 - val_loss: 6.6941 - val_categorical_accuracy: 0.3000 - lr: 3.9063e-06\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3516 - categorical_accuracy: 0.8754\n",
      "Epoch 00020: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.3516 - categorical_accuracy: 0.8754 - val_loss: 7.2335 - val_categorical_accuracy: 0.1833 - lr: 3.9063e-06\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3874 - categorical_accuracy: 0.8651\n",
      "Epoch 00021: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3874 - categorical_accuracy: 0.8651 - val_loss: 5.6438 - val_categorical_accuracy: 0.3167 - lr: 1.9531e-06\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3842 - categorical_accuracy: 0.8651\n",
      "Epoch 00022: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.3842 - categorical_accuracy: 0.8651 - val_loss: 6.3843 - val_categorical_accuracy: 0.2333 - lr: 1.9531e-06\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4015 - categorical_accuracy: 0.8408\n",
      "Epoch 00023: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.4015 - categorical_accuracy: 0.8408 - val_loss: 5.4084 - val_categorical_accuracy: 0.2833 - lr: 9.7656e-07\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4165 - categorical_accuracy: 0.8512\n",
      "Epoch 00024: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.4165 - categorical_accuracy: 0.8512 - val_loss: 4.3957 - val_categorical_accuracy: 0.3167 - lr: 9.7656e-07\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3965 - categorical_accuracy: 0.8478\n",
      "Epoch 00025: val_loss did not improve from 1.46436\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.3965 - categorical_accuracy: 0.8478 - val_loss: 3.6479 - val_categorical_accuracy: 0.3000 - lr: 4.8828e-07\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_2 = model_architecture(25, 40, 120)\n",
    "model_2.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_2.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-2 output summary\n",
    "- Training accuracy: 84.78\n",
    "- Validation accuracy: 30.00\n",
    "\n",
    "We can observe compared to previous model, after training for more number of epochs model validation accuracy did improve and most importantly validation loss went down to 3.6479. Running for more number of epocs will certainly help. \n",
    "\n",
    "Before running for more number of epochs, we should try out with different batch sizes, and image resolution to see the validaton loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-3 input summary (batch size:30)\n",
    "- input image size(120x120)\n",
    "- epochs: 25\n",
    "- batch size: 30\n",
    "\n",
    "In this attempt, try to change the batch size and see how it performed compared to previous model in 25 nb of epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_10 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 30, 120, 120, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 30, 120, 120, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 30, 120, 120, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 15, 60, 60, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_12 (Conv3D)          (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 15, 60, 60, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 7, 30, 30, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 7, 30, 30, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_10 (MaxPoolin  (None, 3, 15, 15, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 3, 15, 15, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_11 (MaxPoolin  (None, 1, 7, 7, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 30\n",
      "Epoch 1/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 7.2428 - categorical_accuracy: 0.2881Source path =  /home/datasets/Project_data/val ; batch size = 30\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 88s 4s/step - loss: 7.2428 - categorical_accuracy: 0.2881 - val_loss: 1.8131 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.2764 - categorical_accuracy: 0.4493\n",
      "Epoch 00002: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 13s 573ms/step - loss: 2.2764 - categorical_accuracy: 0.4493 - val_loss: 5.1867 - val_categorical_accuracy: 0.1250 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.6457 - categorical_accuracy: 0.2899\n",
      "Epoch 00003: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "23/23 [==============================] - 14s 620ms/step - loss: 3.6457 - categorical_accuracy: 0.2899 - val_loss: 2.9905 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.4835 - categorical_accuracy: 0.3188\n",
      "Epoch 00004: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 15s 663ms/step - loss: 3.4835 - categorical_accuracy: 0.3188 - val_loss: 2.9035 - val_categorical_accuracy: 0.1750 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.1055 - categorical_accuracy: 0.4638\n",
      "Epoch 00005: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "23/23 [==============================] - 14s 651ms/step - loss: 2.1055 - categorical_accuracy: 0.4638 - val_loss: 4.0017 - val_categorical_accuracy: 0.2500 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.3338 - categorical_accuracy: 0.3768\n",
      "Epoch 00006: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 13s 596ms/step - loss: 2.3338 - categorical_accuracy: 0.3768 - val_loss: 4.9233 - val_categorical_accuracy: 0.2000 - lr: 2.5000e-04\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7709 - categorical_accuracy: 0.4348\n",
      "Epoch 00007: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "23/23 [==============================] - 15s 664ms/step - loss: 1.7709 - categorical_accuracy: 0.4348 - val_loss: 4.9631 - val_categorical_accuracy: 0.2500 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4188 - categorical_accuracy: 0.4928\n",
      "Epoch 00008: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 1.4188 - categorical_accuracy: 0.4928 - val_loss: 5.1973 - val_categorical_accuracy: 0.2500 - lr: 1.2500e-04\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5215 - categorical_accuracy: 0.5072\n",
      "Epoch 00009: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "23/23 [==============================] - 13s 610ms/step - loss: 1.5215 - categorical_accuracy: 0.5072 - val_loss: 5.3478 - val_categorical_accuracy: 0.3000 - lr: 1.2500e-04\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5610 - categorical_accuracy: 0.5507\n",
      "Epoch 00010: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 13s 601ms/step - loss: 1.5610 - categorical_accuracy: 0.5507 - val_loss: 5.2380 - val_categorical_accuracy: 0.2500 - lr: 6.2500e-05\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9783 - categorical_accuracy: 0.5362\n",
      "Epoch 00011: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "23/23 [==============================] - 14s 638ms/step - loss: 0.9783 - categorical_accuracy: 0.5362 - val_loss: 4.0341 - val_categorical_accuracy: 0.3000 - lr: 6.2500e-05\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9398 - categorical_accuracy: 0.6377\n",
      "Epoch 00012: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 12s 558ms/step - loss: 0.9398 - categorical_accuracy: 0.6377 - val_loss: 4.2049 - val_categorical_accuracy: 0.2750 - lr: 3.1250e-05\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0568 - categorical_accuracy: 0.5942\n",
      "Epoch 00013: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "23/23 [==============================] - 14s 623ms/step - loss: 1.0568 - categorical_accuracy: 0.5942 - val_loss: 3.4508 - val_categorical_accuracy: 0.4000 - lr: 3.1250e-05\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2185 - categorical_accuracy: 0.6087\n",
      "Epoch 00014: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 13s 601ms/step - loss: 1.2185 - categorical_accuracy: 0.6087 - val_loss: 2.9073 - val_categorical_accuracy: 0.4250 - lr: 1.5625e-05\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1080 - categorical_accuracy: 0.6377\n",
      "Epoch 00015: val_loss did not improve from 1.46436\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "23/23 [==============================] - 15s 672ms/step - loss: 1.1080 - categorical_accuracy: 0.6377 - val_loss: 1.9860 - val_categorical_accuracy: 0.4000 - lr: 1.5625e-05\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1941 - categorical_accuracy: 0.6087\n",
      "Epoch 00016: val_loss did not improve from 1.46436\n",
      "23/23 [==============================] - 15s 664ms/step - loss: 1.1941 - categorical_accuracy: 0.6087 - val_loss: 2.4511 - val_categorical_accuracy: 0.4000 - lr: 7.8125e-06\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9597 - categorical_accuracy: 0.5797\n",
      "Epoch 00017: val_loss improved from 1.46436 to 1.39919, saving model to model_init_2022-05-0704_47_36.064098/model-00017-0.95973-0.57971-1.39919-0.55000.h5\n",
      "23/23 [==============================] - 13s 571ms/step - loss: 0.9597 - categorical_accuracy: 0.5797 - val_loss: 1.3992 - val_categorical_accuracy: 0.5500 - lr: 7.8125e-06\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9405 - categorical_accuracy: 0.6522\n",
      "Epoch 00018: val_loss did not improve from 1.39919\n",
      "23/23 [==============================] - 14s 615ms/step - loss: 0.9405 - categorical_accuracy: 0.6522 - val_loss: 1.5122 - val_categorical_accuracy: 0.5750 - lr: 7.8125e-06\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9750 - categorical_accuracy: 0.7246\n",
      "Epoch 00019: val_loss improved from 1.39919 to 1.22284, saving model to model_init_2022-05-0704_47_36.064098/model-00019-0.97500-0.72464-1.22284-0.57500.h5\n",
      "23/23 [==============================] - 14s 618ms/step - loss: 0.9750 - categorical_accuracy: 0.7246 - val_loss: 1.2228 - val_categorical_accuracy: 0.5750 - lr: 7.8125e-06\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0437 - categorical_accuracy: 0.5797\n",
      "Epoch 00020: val_loss improved from 1.22284 to 1.13198, saving model to model_init_2022-05-0704_47_36.064098/model-00020-1.04365-0.57971-1.13198-0.57500.h5\n",
      "23/23 [==============================] - 12s 554ms/step - loss: 1.0437 - categorical_accuracy: 0.5797 - val_loss: 1.1320 - val_categorical_accuracy: 0.5750 - lr: 7.8125e-06\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0525 - categorical_accuracy: 0.5797\n",
      "Epoch 00021: val_loss did not improve from 1.13198\n",
      "23/23 [==============================] - 15s 668ms/step - loss: 1.0525 - categorical_accuracy: 0.5797 - val_loss: 1.1699 - val_categorical_accuracy: 0.6000 - lr: 7.8125e-06\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0207 - categorical_accuracy: 0.6957\n",
      "Epoch 00022: val_loss did not improve from 1.13198\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "23/23 [==============================] - 14s 611ms/step - loss: 1.0207 - categorical_accuracy: 0.6957 - val_loss: 1.1877 - val_categorical_accuracy: 0.5250 - lr: 7.8125e-06\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1212 - categorical_accuracy: 0.5362\n",
      "Epoch 00023: val_loss improved from 1.13198 to 0.78775, saving model to model_init_2022-05-0704_47_36.064098/model-00023-1.12121-0.53623-0.78775-0.67500.h5\n",
      "23/23 [==============================] - 14s 631ms/step - loss: 1.1212 - categorical_accuracy: 0.5362 - val_loss: 0.7877 - val_categorical_accuracy: 0.6750 - lr: 3.9063e-06\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1056 - categorical_accuracy: 0.5797\n",
      "Epoch 00024: val_loss improved from 0.78775 to 0.69709, saving model to model_init_2022-05-0704_47_36.064098/model-00024-1.10558-0.57971-0.69709-0.75000.h5\n",
      "23/23 [==============================] - 13s 597ms/step - loss: 1.1056 - categorical_accuracy: 0.5797 - val_loss: 0.6971 - val_categorical_accuracy: 0.7500 - lr: 3.9063e-06\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1516 - categorical_accuracy: 0.5942\n",
      "Epoch 00025: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 14s 628ms/step - loss: 1.1516 - categorical_accuracy: 0.5942 - val_loss: 1.0932 - val_categorical_accuracy: 0.5500 - lr: 3.9063e-06\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_3 = model_architecture(25, 30, 120)\n",
    "model_3.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_3.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-3 output summary\n",
    "- Training accuracy: 59.42\n",
    "- Validation accuracy: 55.00\n",
    "    \n",
    "##### We can observe that training with batch size of 30 did improve the validation loss and validation accuracy, but training accuracy went down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-4 input summary (Image size 150x150)\n",
    "\n",
    "- input image size(150x150)\n",
    "- epochs: 25\n",
    "- batch size: 30\n",
    "\n",
    "In this attempt, try to change the image width/height size and see how it performed compared to previous model in 25 nb of epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_15 (Conv3D)          (None, 30, 150, 150, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 30, 150, 150, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 30, 150, 150, 8)   0         \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 30, 150, 150, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 30, 150, 150, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_12 (MaxPoolin  (None, 15, 75, 75, 16)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 15, 75, 75, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 15, 75, 75, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_13 (MaxPoolin  (None, 7, 37, 37, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 7, 37, 37, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 7, 37, 37, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_14 (MaxPoolin  (None, 3, 18, 18, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 3, 18, 18, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 3, 18, 18, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_15 (MaxPoolin  (None, 1, 9, 9, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 10368)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1000)              10369000  \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,963,365\n",
      "Trainable params: 10,962,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 11.1899 - categorical_accuracy: 0.3273Source path =  /home/datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 113s 6s/step - loss: 11.1899 - categorical_accuracy: 0.3273 - val_loss: 1.5336 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.2901 - categorical_accuracy: 0.4322\n",
      "Epoch 00002: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 78s 5s/step - loss: 3.2901 - categorical_accuracy: 0.4322 - val_loss: 2.9830 - val_categorical_accuracy: 0.2333 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1246 - categorical_accuracy: 0.5395\n",
      "Epoch 00003: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 69s 4s/step - loss: 2.1246 - categorical_accuracy: 0.5395 - val_loss: 5.2534 - val_categorical_accuracy: 0.1833 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5541 - categorical_accuracy: 0.6037\n",
      "Epoch 00004: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 70s 4s/step - loss: 1.5541 - categorical_accuracy: 0.6037 - val_loss: 9.4528 - val_categorical_accuracy: 0.0667 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1284 - categorical_accuracy: 0.6837\n",
      "Epoch 00005: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 55s 3s/step - loss: 1.1284 - categorical_accuracy: 0.6837 - val_loss: 7.0406 - val_categorical_accuracy: 0.2500 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8370 - categorical_accuracy: 0.7509\n",
      "Epoch 00006: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.8370 - categorical_accuracy: 0.7509 - val_loss: 7.5395 - val_categorical_accuracy: 0.2667 - lr: 2.5000e-04\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7704 - categorical_accuracy: 0.7474\n",
      "Epoch 00007: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.7704 - categorical_accuracy: 0.7474 - val_loss: 9.4297 - val_categorical_accuracy: 0.2667 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6631 - categorical_accuracy: 0.8097\n",
      "Epoch 00008: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.6631 - categorical_accuracy: 0.8097 - val_loss: 10.0096 - val_categorical_accuracy: 0.2333 - lr: 1.2500e-04\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6690 - categorical_accuracy: 0.7958\n",
      "Epoch 00009: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.6690 - categorical_accuracy: 0.7958 - val_loss: 9.4155 - val_categorical_accuracy: 0.2833 - lr: 1.2500e-04\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6671 - categorical_accuracy: 0.7924\n",
      "Epoch 00010: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.6671 - categorical_accuracy: 0.7924 - val_loss: 8.6506 - val_categorical_accuracy: 0.2667 - lr: 6.2500e-05\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4044 - categorical_accuracy: 0.8581\n",
      "Epoch 00011: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4044 - categorical_accuracy: 0.8581 - val_loss: 9.9232 - val_categorical_accuracy: 0.2667 - lr: 6.2500e-05\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5572 - categorical_accuracy: 0.8270\n",
      "Epoch 00012: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.5572 - categorical_accuracy: 0.8270 - val_loss: 7.9615 - val_categorical_accuracy: 0.3333 - lr: 3.1250e-05\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4626 - categorical_accuracy: 0.8408\n",
      "Epoch 00013: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4626 - categorical_accuracy: 0.8408 - val_loss: 9.1217 - val_categorical_accuracy: 0.2333 - lr: 3.1250e-05\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5012 - categorical_accuracy: 0.8408\n",
      "Epoch 00014: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.5012 - categorical_accuracy: 0.8408 - val_loss: 8.1104 - val_categorical_accuracy: 0.2833 - lr: 1.5625e-05\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3791 - categorical_accuracy: 0.8651\n",
      "Epoch 00015: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3791 - categorical_accuracy: 0.8651 - val_loss: 5.2307 - val_categorical_accuracy: 0.4167 - lr: 1.5625e-05\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5752 - categorical_accuracy: 0.8062\n",
      "Epoch 00016: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.5752 - categorical_accuracy: 0.8062 - val_loss: 5.7044 - val_categorical_accuracy: 0.3000 - lr: 7.8125e-06\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4028 - categorical_accuracy: 0.8443\n",
      "Epoch 00017: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4028 - categorical_accuracy: 0.8443 - val_loss: 5.2765 - val_categorical_accuracy: 0.3167 - lr: 7.8125e-06\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4906 - categorical_accuracy: 0.8270\n",
      "Epoch 00018: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.4906 - categorical_accuracy: 0.8270 - val_loss: 5.0103 - val_categorical_accuracy: 0.3000 - lr: 3.9063e-06\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4727 - categorical_accuracy: 0.8235\n",
      "Epoch 00019: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4727 - categorical_accuracy: 0.8235 - val_loss: 3.5673 - val_categorical_accuracy: 0.4333 - lr: 3.9063e-06\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3822 - categorical_accuracy: 0.8616\n",
      "Epoch 00020: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3822 - categorical_accuracy: 0.8616 - val_loss: 3.0109 - val_categorical_accuracy: 0.4000 - lr: 1.9531e-06\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4392 - categorical_accuracy: 0.8304\n",
      "Epoch 00021: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.4392 - categorical_accuracy: 0.8304 - val_loss: 2.5550 - val_categorical_accuracy: 0.4833 - lr: 1.9531e-06\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4359 - categorical_accuracy: 0.8304\n",
      "Epoch 00022: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4359 - categorical_accuracy: 0.8304 - val_loss: 2.7467 - val_categorical_accuracy: 0.4333 - lr: 9.7656e-07\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3388 - categorical_accuracy: 0.8720\n",
      "Epoch 00023: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.3388 - categorical_accuracy: 0.8720 - val_loss: 2.0385 - val_categorical_accuracy: 0.5167 - lr: 9.7656e-07\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4030 - categorical_accuracy: 0.8581\n",
      "Epoch 00024: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.4030 - categorical_accuracy: 0.8581 - val_loss: 1.5049 - val_categorical_accuracy: 0.6167 - lr: 4.8828e-07\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4619 - categorical_accuracy: 0.8408\n",
      "Epoch 00025: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.4619 - categorical_accuracy: 0.8408 - val_loss: 1.3310 - val_categorical_accuracy: 0.6167 - lr: 4.8828e-07\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_4 = model_architecture(25, 40, 150)\n",
    "model_4.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_4.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-4 output summary\n",
    "- Training accuracy: 84.08\n",
    "- Validation accuracy: 61.67\n",
    "     \n",
    "We can see with image size 150x150, training with batch size 40 helps to improve the training/validation accuracy compared with 120x120. We should try to train the model for more number of hours and see the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-5 input summary \n",
    "\n",
    "- input image size(150x150)\n",
    "- epochs: 25\n",
    "- batch size: 30\n",
    "\n",
    "In this attempt, try to change the image width/height size and see how it performed compared to previous model in 25 nb of epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_20 (Conv3D)          (None, 30, 150, 150, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 30, 150, 150, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 30, 150, 150, 8)   0         \n",
      "                                                                 \n",
      " conv3d_21 (Conv3D)          (None, 30, 150, 150, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 30, 150, 150, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_16 (MaxPoolin  (None, 15, 75, 75, 16)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_22 (Conv3D)          (None, 15, 75, 75, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 15, 75, 75, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPoolin  (None, 7, 37, 37, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_23 (Conv3D)          (None, 7, 37, 37, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 7, 37, 37, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 3, 18, 18, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_24 (Conv3D)          (None, 3, 18, 18, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 3, 18, 18, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 1, 9, 9, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 10368)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1000)              10369000  \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,963,365\n",
      "Trainable params: 10,962,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 30\n",
      "Epoch 1/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 9.3676 - categorical_accuracy: 0.3017Source path =  /home/datasets/Project_data/val ; batch size = 30\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 104s 4s/step - loss: 9.3676 - categorical_accuracy: 0.3017 - val_loss: 1.9354 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 4.6302 - categorical_accuracy: 0.3768\n",
      "Epoch 00002: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 15s 680ms/step - loss: 4.6302 - categorical_accuracy: 0.3768 - val_loss: 5.7082 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 5.0723 - categorical_accuracy: 0.2609\n",
      "Epoch 00003: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "23/23 [==============================] - 18s 802ms/step - loss: 5.0723 - categorical_accuracy: 0.2609 - val_loss: 7.7177 - val_categorical_accuracy: 0.1000 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.1792 - categorical_accuracy: 0.5072\n",
      "Epoch 00004: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 14s 631ms/step - loss: 3.1792 - categorical_accuracy: 0.5072 - val_loss: 7.0872 - val_categorical_accuracy: 0.1750 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.1889 - categorical_accuracy: 0.4493\n",
      "Epoch 00005: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "23/23 [==============================] - 16s 739ms/step - loss: 2.1889 - categorical_accuracy: 0.4493 - val_loss: 10.1913 - val_categorical_accuracy: 0.2500 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.3033 - categorical_accuracy: 0.4493\n",
      "Epoch 00006: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 14s 627ms/step - loss: 3.3033 - categorical_accuracy: 0.4493 - val_loss: 11.3674 - val_categorical_accuracy: 0.1250 - lr: 2.5000e-04\n",
      "Epoch 7/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.4373 - categorical_accuracy: 0.4783\n",
      "Epoch 00007: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "23/23 [==============================] - 15s 687ms/step - loss: 2.4373 - categorical_accuracy: 0.4783 - val_loss: 9.3043 - val_categorical_accuracy: 0.2250 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.2010 - categorical_accuracy: 0.4783\n",
      "Epoch 00008: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 14s 628ms/step - loss: 2.2010 - categorical_accuracy: 0.4783 - val_loss: 9.1454 - val_categorical_accuracy: 0.2250 - lr: 1.2500e-04\n",
      "Epoch 9/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.2922 - categorical_accuracy: 0.5942\n",
      "Epoch 00009: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "23/23 [==============================] - 16s 700ms/step - loss: 2.2922 - categorical_accuracy: 0.5942 - val_loss: 4.9076 - val_categorical_accuracy: 0.2750 - lr: 1.2500e-04\n",
      "Epoch 10/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.5186 - categorical_accuracy: 0.5072\n",
      "Epoch 00010: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 16s 724ms/step - loss: 2.5186 - categorical_accuracy: 0.5072 - val_loss: 4.8702 - val_categorical_accuracy: 0.2750 - lr: 6.2500e-05\n",
      "Epoch 11/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9751 - categorical_accuracy: 0.4928\n",
      "Epoch 00011: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "23/23 [==============================] - 14s 648ms/step - loss: 1.9751 - categorical_accuracy: 0.4928 - val_loss: 4.6758 - val_categorical_accuracy: 0.2750 - lr: 6.2500e-05\n",
      "Epoch 12/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3362 - categorical_accuracy: 0.6377\n",
      "Epoch 00012: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 15s 673ms/step - loss: 1.3362 - categorical_accuracy: 0.6377 - val_loss: 4.8066 - val_categorical_accuracy: 0.3000 - lr: 3.1250e-05\n",
      "Epoch 13/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4124 - categorical_accuracy: 0.6957\n",
      "Epoch 00013: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "23/23 [==============================] - 15s 669ms/step - loss: 1.4124 - categorical_accuracy: 0.6957 - val_loss: 4.6441 - val_categorical_accuracy: 0.2250 - lr: 3.1250e-05\n",
      "Epoch 14/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1948 - categorical_accuracy: 0.6087\n",
      "Epoch 00014: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 16s 701ms/step - loss: 1.1948 - categorical_accuracy: 0.6087 - val_loss: 3.3379 - val_categorical_accuracy: 0.3250 - lr: 1.5625e-05\n",
      "Epoch 15/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4295 - categorical_accuracy: 0.6377\n",
      "Epoch 00015: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "23/23 [==============================] - 15s 656ms/step - loss: 1.4295 - categorical_accuracy: 0.6377 - val_loss: 3.0499 - val_categorical_accuracy: 0.3500 - lr: 1.5625e-05\n",
      "Epoch 16/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4500 - categorical_accuracy: 0.6667\n",
      "Epoch 00016: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 15s 679ms/step - loss: 1.4500 - categorical_accuracy: 0.6667 - val_loss: 2.4297 - val_categorical_accuracy: 0.4250 - lr: 7.8125e-06\n",
      "Epoch 17/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6317 - categorical_accuracy: 0.5942\n",
      "Epoch 00017: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "23/23 [==============================] - 14s 649ms/step - loss: 1.6317 - categorical_accuracy: 0.5942 - val_loss: 2.5108 - val_categorical_accuracy: 0.4750 - lr: 7.8125e-06\n",
      "Epoch 18/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5023 - categorical_accuracy: 0.6377\n",
      "Epoch 00018: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 16s 713ms/step - loss: 1.5023 - categorical_accuracy: 0.6377 - val_loss: 2.0425 - val_categorical_accuracy: 0.5250 - lr: 3.9063e-06\n",
      "Epoch 19/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4240 - categorical_accuracy: 0.5942\n",
      "Epoch 00019: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "23/23 [==============================] - 15s 671ms/step - loss: 1.4240 - categorical_accuracy: 0.5942 - val_loss: 2.2071 - val_categorical_accuracy: 0.4250 - lr: 3.9063e-06\n",
      "Epoch 20/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4791 - categorical_accuracy: 0.6957\n",
      "Epoch 00020: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 15s 679ms/step - loss: 1.4791 - categorical_accuracy: 0.6957 - val_loss: 1.6674 - val_categorical_accuracy: 0.5500 - lr: 1.9531e-06\n",
      "Epoch 21/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1401 - categorical_accuracy: 0.6087\n",
      "Epoch 00021: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 15s 697ms/step - loss: 1.1401 - categorical_accuracy: 0.6087 - val_loss: 1.7925 - val_categorical_accuracy: 0.5250 - lr: 1.9531e-06\n",
      "Epoch 22/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3070 - categorical_accuracy: 0.6232\n",
      "Epoch 00022: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 15s 686ms/step - loss: 1.3070 - categorical_accuracy: 0.6232 - val_loss: 1.5375 - val_categorical_accuracy: 0.5250 - lr: 1.9531e-06\n",
      "Epoch 23/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2240 - categorical_accuracy: 0.6522\n",
      "Epoch 00023: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 14s 644ms/step - loss: 1.2240 - categorical_accuracy: 0.6522 - val_loss: 1.1749 - val_categorical_accuracy: 0.6500 - lr: 1.9531e-06\n",
      "Epoch 24/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4131 - categorical_accuracy: 0.6957\n",
      "Epoch 00024: val_loss did not improve from 0.69709\n",
      "23/23 [==============================] - 15s 686ms/step - loss: 1.4131 - categorical_accuracy: 0.6957 - val_loss: 1.4527 - val_categorical_accuracy: 0.6250 - lr: 1.9531e-06\n",
      "Epoch 25/25\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5052 - categorical_accuracy: 0.5507\n",
      "Epoch 00025: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "23/23 [==============================] - 13s 609ms/step - loss: 1.5052 - categorical_accuracy: 0.5507 - val_loss: 1.2163 - val_categorical_accuracy: 0.6500 - lr: 1.9531e-06\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_5 = model_architecture(25, 30, 150)\n",
    "model_5.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_5.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-5 output summary\n",
    "- Training accuracy: 55.07\n",
    "- Validation accuracy: 65.00\n",
    "\n",
    "We can observe that training the model with less batch size brings down the training accuracy and improves validation loss. Though the validation loss comes down but training with more number of batches will help to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By running different models, we can observe that:\n",
    "- increasing the batch size definitely helps (40 vs 30)\n",
    "- image size 150x150 with 25 epochs perform better compared to 120x120. It results in better validation loss and training & validation accuracy\n",
    "\n",
    "We should experiment the model with more number of samples and higer epochs to see the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-6 input summary (Image size: 120, with more number of samples in a batch size)\n",
    "\n",
    "- input image size(120x120)\n",
    "- epochs: 25\n",
    "- batch size: 50\n",
    "\n",
    "In this attempt, try to change the image width/height size and see how it performed compared to previous model in 25 nb of epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_25 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 30, 120, 120, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " conv3d_26 (Conv3D)          (None, 30, 120, 120, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 30, 120, 120, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_20 (MaxPoolin  (None, 15, 60, 60, 16)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 15, 60, 60, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_21 (MaxPoolin  (None, 7, 30, 30, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_28 (Conv3D)          (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 7, 30, 30, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_22 (MaxPoolin  (None, 3, 15, 15, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 3, 15, 15, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_23 (MaxPoolin  (None, 1, 7, 7, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 7.6897 - categorical_accuracy: 0.2670Source path =  /home/datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 99s 7s/step - loss: 7.6897 - categorical_accuracy: 0.2670 - val_loss: 2.0197 - val_categorical_accuracy: 0.2467 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.3134 - categorical_accuracy: 0.4286\n",
      "Epoch 00002: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.3134 - categorical_accuracy: 0.4286 - val_loss: 1.6716 - val_categorical_accuracy: 0.2733 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.4065 - categorical_accuracy: 0.3846\n",
      "Epoch 00003: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 46s 3s/step - loss: 2.4065 - categorical_accuracy: 0.3846 - val_loss: 2.4286 - val_categorical_accuracy: 0.2733 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 2.0104 - categorical_accuracy: 0.4231\n",
      "Epoch 00004: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 44s 3s/step - loss: 2.0104 - categorical_accuracy: 0.4231 - val_loss: 2.8151 - val_categorical_accuracy: 0.2267 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4712 - categorical_accuracy: 0.4890\n",
      "Epoch 00005: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.4712 - categorical_accuracy: 0.4890 - val_loss: 3.3970 - val_categorical_accuracy: 0.1733 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3555 - categorical_accuracy: 0.5769\n",
      "Epoch 00006: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 51s 4s/step - loss: 1.3555 - categorical_accuracy: 0.5769 - val_loss: 3.8463 - val_categorical_accuracy: 0.1600 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9949 - categorical_accuracy: 0.6593\n",
      "Epoch 00007: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.9949 - categorical_accuracy: 0.6593 - val_loss: 4.6708 - val_categorical_accuracy: 0.1400 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0341 - categorical_accuracy: 0.6209\n",
      "Epoch 00008: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 46s 4s/step - loss: 1.0341 - categorical_accuracy: 0.6209 - val_loss: 4.7585 - val_categorical_accuracy: 0.1533 - lr: 2.5000e-04\n",
      "Epoch 9/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9309 - categorical_accuracy: 0.6648\n",
      "Epoch 00009: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.9309 - categorical_accuracy: 0.6648 - val_loss: 4.6175 - val_categorical_accuracy: 0.1600 - lr: 1.2500e-04\n",
      "Epoch 10/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9053 - categorical_accuracy: 0.6813\n",
      "Epoch 00010: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.9053 - categorical_accuracy: 0.6813 - val_loss: 4.4594 - val_categorical_accuracy: 0.1600 - lr: 1.2500e-04\n",
      "Epoch 11/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7911 - categorical_accuracy: 0.7253\n",
      "Epoch 00011: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.7911 - categorical_accuracy: 0.7253 - val_loss: 4.5118 - val_categorical_accuracy: 0.1400 - lr: 6.2500e-05\n",
      "Epoch 12/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6245 - categorical_accuracy: 0.7418\n",
      "Epoch 00012: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.6245 - categorical_accuracy: 0.7418 - val_loss: 4.4770 - val_categorical_accuracy: 0.1600 - lr: 6.2500e-05\n",
      "Epoch 13/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7017 - categorical_accuracy: 0.6978\n",
      "Epoch 00013: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.7017 - categorical_accuracy: 0.6978 - val_loss: 4.5490 - val_categorical_accuracy: 0.1533 - lr: 3.1250e-05\n",
      "Epoch 14/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5983 - categorical_accuracy: 0.7692\n",
      "Epoch 00014: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "14/14 [==============================] - 46s 4s/step - loss: 0.5983 - categorical_accuracy: 0.7692 - val_loss: 4.5890 - val_categorical_accuracy: 0.1933 - lr: 3.1250e-05\n",
      "Epoch 15/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7238 - categorical_accuracy: 0.7088\n",
      "Epoch 00015: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.7238 - categorical_accuracy: 0.7088 - val_loss: 4.3967 - val_categorical_accuracy: 0.2200 - lr: 1.5625e-05\n",
      "Epoch 16/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6844 - categorical_accuracy: 0.7198\n",
      "Epoch 00016: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.6844 - categorical_accuracy: 0.7198 - val_loss: 4.2344 - val_categorical_accuracy: 0.2200 - lr: 1.5625e-05\n",
      "Epoch 17/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6382 - categorical_accuracy: 0.7143\n",
      "Epoch 00017: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.6382 - categorical_accuracy: 0.7143 - val_loss: 4.2070 - val_categorical_accuracy: 0.2067 - lr: 7.8125e-06\n",
      "Epoch 18/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6888 - categorical_accuracy: 0.7033\n",
      "Epoch 00018: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.6888 - categorical_accuracy: 0.7033 - val_loss: 4.0075 - val_categorical_accuracy: 0.2467 - lr: 7.8125e-06\n",
      "Epoch 19/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7098 - categorical_accuracy: 0.7527\n",
      "Epoch 00019: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.7098 - categorical_accuracy: 0.7527 - val_loss: 3.8099 - val_categorical_accuracy: 0.2467 - lr: 3.9063e-06\n",
      "Epoch 20/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6561 - categorical_accuracy: 0.7363\n",
      "Epoch 00020: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.6561 - categorical_accuracy: 0.7363 - val_loss: 3.4804 - val_categorical_accuracy: 0.2467 - lr: 3.9063e-06\n",
      "Epoch 21/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5942 - categorical_accuracy: 0.7473\n",
      "Epoch 00021: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.5942 - categorical_accuracy: 0.7473 - val_loss: 3.4126 - val_categorical_accuracy: 0.2467 - lr: 1.9531e-06\n",
      "Epoch 22/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6525 - categorical_accuracy: 0.7582\n",
      "Epoch 00022: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.6525 - categorical_accuracy: 0.7582 - val_loss: 3.0404 - val_categorical_accuracy: 0.2733 - lr: 1.9531e-06\n",
      "Epoch 23/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6147 - categorical_accuracy: 0.7473\n",
      "Epoch 00023: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 44s 3s/step - loss: 0.6147 - categorical_accuracy: 0.7473 - val_loss: 2.8886 - val_categorical_accuracy: 0.2800 - lr: 9.7656e-07\n",
      "Epoch 24/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6414 - categorical_accuracy: 0.7418\n",
      "Epoch 00024: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.6414 - categorical_accuracy: 0.7418 - val_loss: 2.5814 - val_categorical_accuracy: 0.3267 - lr: 9.7656e-07\n",
      "Epoch 25/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6493 - categorical_accuracy: 0.7143\n",
      "Epoch 00025: val_loss did not improve from 0.69709\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.6493 - categorical_accuracy: 0.7143 - val_loss: 2.2939 - val_categorical_accuracy: 0.3800 - lr: 4.8828e-07\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_6 = model_architecture(25, 50, 120)\n",
    "model_6.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_6.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-6 output summary\n",
    "- Training accuracy: 72.43\n",
    "- Validation accuracy: 38.00\n",
    "\n",
    "We can observe that training the model with more number of batch sizes brings down the validation loss and improved the validation accuracy. But the training accuracy went down. Later we try to run with more number of epocs to see the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-7 input summary (Image size: 150, batch size = 50)\n",
    "\n",
    "- input image size(150x150)\n",
    "- epochs: 25\n",
    "- batch size: 50\n",
    "\n",
    "##### With higher number of batches, we can notice OOM error. With further resource optimization and increased processing power model can be trained and behavior can be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_30 (Conv3D)          (None, 30, 150, 150, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 30, 150, 150, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 30, 150, 150, 8)   0         \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 30, 150, 150, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 30, 150, 150, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_24 (MaxPoolin  (None, 15, 75, 75, 16)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_32 (Conv3D)          (None, 15, 75, 75, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 15, 75, 75, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_25 (MaxPoolin  (None, 7, 37, 37, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_33 (Conv3D)          (None, 7, 37, 37, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 7, 37, 37, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_26 (MaxPoolin  (None, 3, 18, 18, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_34 (Conv3D)          (None, 3, 18, 18, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 3, 18, 18, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_27 (MaxPoolin  (None, 1, 9, 9, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 10368)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1000)              10369000  \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,963,365\n",
      "Trainable params: 10,962,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-07 06:22:31.331946: W tensorflow/core/common_runtime/bfc_allocator.cc:463] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.01GiB (rounded to 2160000000)requested by op gradient_tape/sequential_6/max_pooling3d_24/MaxPool3D/MaxPool3DGrad\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-05-07 06:22:31.332115: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2022-05-07 06:22:31.332159: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 474, Chunks in use: 473. 118.5KiB allocated for chunks. 118.2KiB in use in bin. 37.4KiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332186: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 78, Chunks in use: 77. 39.8KiB allocated for chunks. 39.0KiB in use in bin. 38.5KiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332209: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 3, Chunks in use: 1. 3.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332235: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 42, Chunks in use: 42. 104.2KiB allocated for chunks. 104.2KiB in use in bin. 94.2KiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332260: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 21, Chunks in use: 21. 84.2KiB allocated for chunks. 84.2KiB in use in bin. 82.0KiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332286: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 32, Chunks in use: 32. 383.5KiB allocated for chunks. 383.5KiB in use in bin. 379.7KiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332311: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 31, Chunks in use: 31. 529.2KiB allocated for chunks. 529.2KiB in use in bin. 444.9KiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332334: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332358: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 22, Chunks in use: 21. 1.65MiB allocated for chunks. 1.53MiB in use in bin. 1.31MiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332378: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332400: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 21, Chunks in use: 21. 5.69MiB allocated for chunks. 5.69MiB in use in bin. 5.25MiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332420: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332447: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 21, Chunks in use: 20. 39.81MiB allocated for chunks. 38.15MiB in use in bin. 38.15MiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332469: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 2.44MiB allocated for chunks. 2.44MiB in use in bin. 1.91MiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332489: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332509: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332535: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 10, Chunks in use: 10. 245.78MiB allocated for chunks. 245.78MiB in use in bin. 239.26MiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332560: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 8, Chunks in use: 8. 308.88MiB allocated for chunks. 308.88MiB in use in bin. 285.16MiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332592: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 3, Chunks in use: 3. 221.56MiB allocated for chunks. 221.56MiB in use in bin. 118.65MiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332615: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 128.75MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332640: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 12, Chunks in use: 8. 13.52GiB allocated for chunks. 9.01GiB in use in bin. 8.93GiB client-requested in use in bin.\n",
      "2022-05-07 06:22:31.332663: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 2.01GiB was 256.00MiB, Chunk State: \n",
      "2022-05-07 06:22:31.332698: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 724.06MiB | Requested Size: 257.49MiB | in_use: 0 | bin_num: 20, prev:   Size: 282.03MiB | Requested Size: 257.49MiB | in_use: 1 | bin_num: -1\n",
      "2022-05-07 06:22:31.332725: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 1018.03MiB | Requested Size: 514.98MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.00GiB | Requested Size: 1.00GiB | in_use: 1 | bin_num: -1\n",
      "2022-05-07 06:22:31.332748: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 1.00GiB | Requested Size: 514.98MiB | in_use: 0 | bin_num: 20\n",
      "2022-05-07 06:22:31.332774: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 1.81GiB | Requested Size: 514.98MiB | in_use: 0 | bin_num: 20, prev:   Size: 2.01GiB | Requested Size: 2.01GiB | in_use: 1 | bin_num: -1\n",
      "2022-05-07 06:22:31.332791: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 6259277824\n",
      "2022-05-07 06:22:31.332812: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fa912000000 of size 2160000000 next 772\n",
      "2022-05-07 06:22:31.332830: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fa992befc00 of size 2160000000 next 775\n",
      "2022-05-07 06:22:31.332848: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7faa137df800 of size 1939277824 next 18446744073709551615\n",
      "2022-05-07 06:22:31.332864: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4294967296\n",
      "2022-05-07 06:22:31.332882: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7faac4000000 of size 1080000000 next 767\n",
      "2022-05-07 06:22:31.332899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fab045f7e00 of size 2160000000 next 771\n",
      "2022-05-07 06:22:31.332917: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fab851e7a00 of size 295731200 next 777\n",
      "2022-05-07 06:22:31.332933: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fab96befa00 of size 759236096 next 18446744073709551615\n",
      "2022-05-07 06:22:31.332950: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2147483648\n",
      "2022-05-07 06:22:31.332967: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fabca000000 of size 1080000000 next 766\n",
      "2022-05-07 06:22:31.332984: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fac0a5f7e00 of size 1067483648 next 18446744073709551615\n",
      "2022-05-07 06:22:31.333001: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1073741824\n",
      "2022-05-07 06:22:31.333018: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8a000000 of size 25088000 next 570\n",
      "2022-05-07 06:22:31.333035: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ed000 of size 256 next 672\n",
      "2022-05-07 06:22:31.333053: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ed100 of size 2816 next 706\n",
      "2022-05-07 06:22:31.333069: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7edc00 of size 256 next 694\n",
      "2022-05-07 06:22:31.333089: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7edd00 of size 256 next 663\n",
      "2022-05-07 06:22:31.333107: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ede00 of size 256 next 685\n",
      "2022-05-07 06:22:31.333124: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7edf00 of size 256 next 593\n",
      "2022-05-07 06:22:31.333140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee000 of size 256 next 697\n",
      "2022-05-07 06:22:31.333157: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee100 of size 256 next 708\n",
      "2022-05-07 06:22:31.333178: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee200 of size 256 next 710\n",
      "2022-05-07 06:22:31.333195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee300 of size 256 next 711\n",
      "2022-05-07 06:22:31.333211: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee400 of size 256 next 712\n",
      "2022-05-07 06:22:31.333227: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee500 of size 256 next 713\n",
      "2022-05-07 06:22:31.333244: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee600 of size 256 next 714\n",
      "2022-05-07 06:22:31.333261: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee700 of size 256 next 715\n",
      "2022-05-07 06:22:31.333278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ee800 of size 512 next 717\n",
      "2022-05-07 06:22:31.333295: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7eea00 of size 512 next 718\n",
      "2022-05-07 06:22:31.333312: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7eec00 of size 512 next 719\n",
      "2022-05-07 06:22:31.333330: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7eee00 of size 2560 next 677\n",
      "2022-05-07 06:22:31.333347: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ef800 of size 17408 next 666\n",
      "2022-05-07 06:22:31.333363: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7f3c00 of size 13824 next 660\n",
      "2022-05-07 06:22:31.333382: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7f7200 of size 2048 next 671\n",
      "2022-05-07 06:22:31.333400: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7f7a00 of size 14336 next 662\n",
      "2022-05-07 06:22:31.333417: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7fb200 of size 16384 next 664\n",
      "2022-05-07 06:22:31.333434: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b7ff200 of size 16384 next 709\n",
      "2022-05-07 06:22:31.333452: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b803200 of size 114688 next 686\n",
      "2022-05-07 06:22:31.333469: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b81f200 of size 65536 next 691\n",
      "2022-05-07 06:22:31.333485: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b82f200 of size 262144 next 716\n",
      "2022-05-07 06:22:31.333502: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b86f200 of size 4096 next 720\n",
      "2022-05-07 06:22:31.333520: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b870200 of size 10240 next 722\n",
      "2022-05-07 06:22:31.333537: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b872a00 of size 256 next 723\n",
      "2022-05-07 06:22:31.333554: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b872b00 of size 2816 next 724\n",
      "2022-05-07 06:22:31.333571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b873600 of size 256 next 725\n",
      "2022-05-07 06:22:31.333588: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b873700 of size 256 next 726\n",
      "2022-05-07 06:22:31.333604: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b873800 of size 256 next 727\n",
      "2022-05-07 06:22:31.333620: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b873900 of size 13824 next 728\n",
      "2022-05-07 06:22:31.333639: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b876f00 of size 256 next 729\n",
      "2022-05-07 06:22:31.333656: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b877000 of size 256 next 730\n",
      "2022-05-07 06:22:31.333673: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b877100 of size 256 next 731\n",
      "2022-05-07 06:22:31.333690: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b877200 of size 16384 next 732\n",
      "2022-05-07 06:22:31.333706: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b87b200 of size 256 next 733\n",
      "2022-05-07 06:22:31.333724: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b87b300 of size 256 next 734\n",
      "2022-05-07 06:22:31.333740: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b87b400 of size 256 next 735\n",
      "2022-05-07 06:22:31.333756: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b87b500 of size 65536 next 736\n",
      "2022-05-07 06:22:31.333772: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88b500 of size 256 next 737\n",
      "2022-05-07 06:22:31.333789: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88b600 of size 256 next 738\n",
      "2022-05-07 06:22:31.333806: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88b700 of size 256 next 739\n",
      "2022-05-07 06:22:31.333822: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88b800 of size 512 next 741\n",
      "2022-05-07 06:22:31.333838: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88ba00 of size 512 next 742\n",
      "2022-05-07 06:22:31.333855: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88bc00 of size 512 next 743\n",
      "2022-05-07 06:22:31.333872: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88be00 of size 4096 next 745\n",
      "2022-05-07 06:22:31.333889: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88ce00 of size 2048 next 747\n",
      "2022-05-07 06:22:31.333905: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88d600 of size 10240 next 748\n",
      "2022-05-07 06:22:31.333922: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88fe00 of size 256 next 749\n",
      "2022-05-07 06:22:31.333938: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b88ff00 of size 256 next 750\n",
      "2022-05-07 06:22:31.333954: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890000 of size 256 next 751\n",
      "2022-05-07 06:22:31.333970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890100 of size 256 next 752\n",
      "2022-05-07 06:22:31.333987: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890200 of size 256 next 753\n",
      "2022-05-07 06:22:31.334003: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890300 of size 256 next 754\n",
      "2022-05-07 06:22:31.334020: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890400 of size 256 next 755\n",
      "2022-05-07 06:22:31.334036: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890500 of size 256 next 756\n",
      "2022-05-07 06:22:31.334053: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890600 of size 256 next 757\n",
      "2022-05-07 06:22:31.334069: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890700 of size 256 next 758\n",
      "2022-05-07 06:22:31.334085: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890800 of size 256 next 759\n",
      "2022-05-07 06:22:31.334101: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890900 of size 256 next 760\n",
      "2022-05-07 06:22:31.334118: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890a00 of size 256 next 803\n",
      "2022-05-07 06:22:31.334135: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fac8b890b00 of size 768 next 761\n",
      "2022-05-07 06:22:31.334151: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b890e00 of size 256 next 763\n",
      "2022-05-07 06:22:31.334167: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fac8b890f00 of size 256 next 764\n",
      "2022-05-07 06:22:31.334184: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891000 of size 256 next 800\n",
      "2022-05-07 06:22:31.334201: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891100 of size 256 next 765\n",
      "2022-05-07 06:22:31.334216: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891200 of size 256 next 768\n",
      "2022-05-07 06:22:31.334233: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891300 of size 256 next 769\n",
      "2022-05-07 06:22:31.334249: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891400 of size 256 next 770\n",
      "2022-05-07 06:22:31.334266: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891500 of size 256 next 773\n",
      "2022-05-07 06:22:31.334282: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891600 of size 256 next 774\n",
      "2022-05-07 06:22:31.334298: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fac8b891700 of size 1024 next 785\n",
      "2022-05-07 06:22:31.334315: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b891b00 of size 512 next 790\n",
      "2022-05-07 06:22:31.334332: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fac8b891d00 of size 120064 next 665\n",
      "2022-05-07 06:22:31.334349: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b8af200 of size 262144 next 668\n",
      "2022-05-07 06:22:31.334368: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8b8ef200 of size 2000128 next 721\n",
      "2022-05-07 06:22:31.334386: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8bad7700 of size 262144 next 740\n",
      "2022-05-07 06:22:31.334402: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fac8bb17700 of size 1737984 next 687\n",
      "2022-05-07 06:22:31.334418: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8bcbfc00 of size 2000128 next 692\n",
      "2022-05-07 06:22:31.334435: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac8bea8100 of size 76943616 next 577\n",
      "2022-05-07 06:22:31.334454: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac90809200 of size 41472000 next 695\n",
      "2022-05-07 06:22:31.334471: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac92f96200 of size 41472000 next 744\n",
      "2022-05-07 06:22:31.334488: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac95723200 of size 2000128 next 746\n",
      "2022-05-07 06:22:31.334505: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fac9590b700 of size 270000128 next 701\n",
      "2022-05-07 06:22:31.334522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7faca5a89700 of size 135000064 next 762\n",
      "2022-05-07 06:22:31.334540: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7facadb48700 of size 474708224 next 18446744073709551615\n",
      "2022-05-07 06:22:31.334557: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1073741824\n",
      "2022-05-07 06:22:31.334574: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fad42000000 of size 1073741824 next 18446744073709551615\n",
      "2022-05-07 06:22:31.334591: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 268435456\n",
      "2022-05-07 06:22:31.334608: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4000000 of size 2000128 next 203\n",
      "2022-05-07 06:22:31.334625: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada41e8500 of size 262144 next 222\n",
      "2022-05-07 06:22:31.334641: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4228500 of size 13824 next 293\n",
      "2022-05-07 06:22:31.334658: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada422bb00 of size 16384 next 297\n",
      "2022-05-07 06:22:31.334675: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada422fb00 of size 10240 next 311\n",
      "2022-05-07 06:22:31.334691: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4232300 of size 25088 next 243\n",
      "2022-05-07 06:22:31.334708: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4238500 of size 65536 next 284\n",
      "2022-05-07 06:22:31.334725: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4248500 of size 262144 next 304\n",
      "2022-05-07 06:22:31.334742: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4288500 of size 16384 next 320\n",
      "2022-05-07 06:22:31.334759: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada428c500 of size 65536 next 324\n",
      "2022-05-07 06:22:31.334775: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada429c500 of size 4096 next 332\n",
      "2022-05-07 06:22:31.334792: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada429d500 of size 2048 next 333\n",
      "2022-05-07 06:22:31.334809: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada429dd00 of size 10240 next 334\n",
      "2022-05-07 06:22:31.334825: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0500 of size 256 next 336\n",
      "2022-05-07 06:22:31.334841: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0600 of size 256 next 337\n",
      "2022-05-07 06:22:31.334859: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0700 of size 256 next 338\n",
      "2022-05-07 06:22:31.334876: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0800 of size 256 next 339\n",
      "2022-05-07 06:22:31.334892: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0900 of size 256 next 340\n",
      "2022-05-07 06:22:31.334908: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0a00 of size 256 next 341\n",
      "2022-05-07 06:22:31.334925: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0b00 of size 256 next 342\n",
      "2022-05-07 06:22:31.334941: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0c00 of size 256 next 343\n",
      "2022-05-07 06:22:31.334957: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0d00 of size 256 next 368\n",
      "2022-05-07 06:22:31.334973: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0e00 of size 256 next 357\n",
      "2022-05-07 06:22:31.334990: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a0f00 of size 256 next 363\n",
      "2022-05-07 06:22:31.335006: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1000 of size 256 next 382\n",
      "2022-05-07 06:22:31.335023: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1100 of size 256 next 346\n",
      "2022-05-07 06:22:31.335039: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1200 of size 256 next 360\n",
      "2022-05-07 06:22:31.335056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1300 of size 256 next 351\n",
      "2022-05-07 06:22:31.335072: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1400 of size 256 next 154\n",
      "2022-05-07 06:22:31.335088: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1500 of size 256 next 472\n",
      "2022-05-07 06:22:31.335104: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1600 of size 256 next 348\n",
      "2022-05-07 06:22:31.335121: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1700 of size 512 next 371\n",
      "2022-05-07 06:22:31.335137: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1900 of size 512 next 385\n",
      "2022-05-07 06:22:31.335154: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1b00 of size 256 next 352\n",
      "2022-05-07 06:22:31.335170: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1c00 of size 256 next 454\n",
      "2022-05-07 06:22:31.335187: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1d00 of size 256 next 358\n",
      "2022-05-07 06:22:31.335203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1e00 of size 256 next 390\n",
      "2022-05-07 06:22:31.335219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a1f00 of size 256 next 355\n",
      "2022-05-07 06:22:31.335235: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2000 of size 256 next 276\n",
      "2022-05-07 06:22:31.335252: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2100 of size 256 next 377\n",
      "2022-05-07 06:22:31.335269: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2200 of size 256 next 366\n",
      "2022-05-07 06:22:31.335285: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2300 of size 256 next 488\n",
      "2022-05-07 06:22:31.335301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2400 of size 256 next 393\n",
      "2022-05-07 06:22:31.335318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2500 of size 512 next 381\n",
      "2022-05-07 06:22:31.335335: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2700 of size 512 next 364\n",
      "2022-05-07 06:22:31.335351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2900 of size 512 next 388\n",
      "2022-05-07 06:22:31.335367: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2b00 of size 256 next 347\n",
      "2022-05-07 06:22:31.335384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2c00 of size 256 next 482\n",
      "2022-05-07 06:22:31.335400: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2d00 of size 256 next 484\n",
      "2022-05-07 06:22:31.335416: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2e00 of size 256 next 344\n",
      "2022-05-07 06:22:31.335432: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a2f00 of size 256 next 374\n",
      "2022-05-07 06:22:31.335448: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a3000 of size 256 next 353\n",
      "2022-05-07 06:22:31.335465: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a3100 of size 256 next 384\n",
      "2022-05-07 06:22:31.335481: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a3200 of size 256 next 359\n",
      "2022-05-07 06:22:31.335498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a3300 of size 256 next 380\n",
      "2022-05-07 06:22:31.335542: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a3400 of size 256 next 379\n",
      "2022-05-07 06:22:31.335559: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a3500 of size 256 next 392\n",
      "2022-05-07 06:22:31.335574: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a3600 of size 2816 next 378\n",
      "2022-05-07 06:22:31.335592: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a4100 of size 2816 next 345\n",
      "2022-05-07 06:22:31.335609: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a4c00 of size 256 next 396\n",
      "2022-05-07 06:22:31.335624: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a4d00 of size 256 next 397\n",
      "2022-05-07 06:22:31.335640: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a4e00 of size 256 next 399\n",
      "2022-05-07 06:22:31.335657: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a4f00 of size 256 next 400\n",
      "2022-05-07 06:22:31.335674: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5000 of size 256 next 401\n",
      "2022-05-07 06:22:31.335690: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5100 of size 256 next 403\n",
      "2022-05-07 06:22:31.335706: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5200 of size 256 next 404\n",
      "2022-05-07 06:22:31.335723: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5300 of size 256 next 405\n",
      "2022-05-07 06:22:31.335739: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5400 of size 256 next 406\n",
      "2022-05-07 06:22:31.335755: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5500 of size 256 next 407\n",
      "2022-05-07 06:22:31.335771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5600 of size 256 next 408\n",
      "2022-05-07 06:22:31.335788: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5700 of size 512 next 409\n",
      "2022-05-07 06:22:31.335805: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5900 of size 512 next 410\n",
      "2022-05-07 06:22:31.335821: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5b00 of size 512 next 411\n",
      "2022-05-07 06:22:31.335838: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a5d00 of size 3072 next 373\n",
      "2022-05-07 06:22:31.335856: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42a6900 of size 17408 next 349\n",
      "2022-05-07 06:22:31.335873: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42aad00 of size 13824 next 354\n",
      "2022-05-07 06:22:31.335889: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42ae300 of size 4096 next 365\n",
      "2022-05-07 06:22:31.335905: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42af300 of size 2048 next 362\n",
      "2022-05-07 06:22:31.335922: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42afb00 of size 4096 next 413\n",
      "2022-05-07 06:22:31.335939: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b0b00 of size 256 next 416\n",
      "2022-05-07 06:22:31.335956: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b0c00 of size 2816 next 417\n",
      "2022-05-07 06:22:31.335972: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1700 of size 256 next 418\n",
      "2022-05-07 06:22:31.335989: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1800 of size 256 next 419\n",
      "2022-05-07 06:22:31.336005: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1900 of size 256 next 420\n",
      "2022-05-07 06:22:31.336021: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1a00 of size 256 next 421\n",
      "2022-05-07 06:22:31.336036: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1b00 of size 256 next 422\n",
      "2022-05-07 06:22:31.336053: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1c00 of size 256 next 423\n",
      "2022-05-07 06:22:31.336070: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1d00 of size 256 next 425\n",
      "2022-05-07 06:22:31.336086: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1e00 of size 256 next 426\n",
      "2022-05-07 06:22:31.336102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b1f00 of size 256 next 427\n",
      "2022-05-07 06:22:31.336120: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b2000 of size 256 next 429\n",
      "2022-05-07 06:22:31.336136: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b2100 of size 256 next 430\n",
      "2022-05-07 06:22:31.336152: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b2200 of size 256 next 369\n",
      "2022-05-07 06:22:31.336167: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b2300 of size 16384 next 350\n",
      "2022-05-07 06:22:31.336186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42b6300 of size 74240 next 281\n",
      "2022-05-07 06:22:31.336203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada42c8500 of size 262144 next 184\n",
      "2022-05-07 06:22:31.336220: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4308500 of size 262144 next 328\n",
      "2022-05-07 06:22:31.336236: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4348500 of size 13824 next 398\n",
      "2022-05-07 06:22:31.336253: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada434bb00 of size 16384 next 402\n",
      "2022-05-07 06:22:31.336270: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada434fb00 of size 10240 next 415\n",
      "2022-05-07 06:22:31.336286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4352300 of size 25088 next 386\n",
      "2022-05-07 06:22:31.336302: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4358500 of size 65536 next 253\n",
      "2022-05-07 06:22:31.336319: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4368500 of size 16384 next 424\n",
      "2022-05-07 06:22:31.336336: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada436c500 of size 65536 next 428\n",
      "2022-05-07 06:22:31.336354: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada437c500 of size 345344 next 30\n",
      "2022-05-07 06:22:31.336371: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada43d0a00 of size 2000128 next 155\n",
      "2022-05-07 06:22:31.336388: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada45b8f00 of size 2000128 next 228\n",
      "2022-05-07 06:22:31.336404: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada47a1400 of size 2000128 next 310\n",
      "2022-05-07 06:22:31.336420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4989900 of size 2000128 next 250\n",
      "2022-05-07 06:22:31.336436: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4b71e00 of size 2000128 next 249\n",
      "2022-05-07 06:22:31.336453: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4d5a300 of size 262144 next 389\n",
      "2022-05-07 06:22:31.336470: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4d9a300 of size 262144 next 395\n",
      "2022-05-07 06:22:31.336486: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4dda300 of size 2000128 next 414\n",
      "2022-05-07 06:22:31.336502: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc2800 of size 512 next 431\n",
      "2022-05-07 06:22:31.336519: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc2a00 of size 512 next 432\n",
      "2022-05-07 06:22:31.336536: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc2c00 of size 512 next 433\n",
      "2022-05-07 06:22:31.336552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc2e00 of size 4096 next 434\n",
      "2022-05-07 06:22:31.336568: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc3e00 of size 2048 next 436\n",
      "2022-05-07 06:22:31.336585: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc4600 of size 10240 next 437\n",
      "2022-05-07 06:22:31.336602: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc6e00 of size 256 next 438\n",
      "2022-05-07 06:22:31.336618: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc6f00 of size 256 next 439\n",
      "2022-05-07 06:22:31.336633: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7000 of size 256 next 440\n",
      "2022-05-07 06:22:31.336651: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7100 of size 256 next 441\n",
      "2022-05-07 06:22:31.336667: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7200 of size 256 next 442\n",
      "2022-05-07 06:22:31.336683: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7300 of size 256 next 443\n",
      "2022-05-07 06:22:31.336699: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7400 of size 256 next 444\n",
      "2022-05-07 06:22:31.336716: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7500 of size 256 next 445\n",
      "2022-05-07 06:22:31.336732: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7600 of size 256 next 446\n",
      "2022-05-07 06:22:31.336748: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7700 of size 256 next 447\n",
      "2022-05-07 06:22:31.336764: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7800 of size 256 next 481\n",
      "2022-05-07 06:22:31.336781: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7900 of size 256 next 485\n",
      "2022-05-07 06:22:31.336798: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7a00 of size 256 next 471\n",
      "2022-05-07 06:22:31.336817: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7b00 of size 256 next 480\n",
      "2022-05-07 06:22:31.336833: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7c00 of size 256 next 468\n",
      "2022-05-07 06:22:31.336850: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7d00 of size 256 next 586\n",
      "2022-05-07 06:22:31.336867: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7e00 of size 256 next 469\n",
      "2022-05-07 06:22:31.336882: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc7f00 of size 256 next 467\n",
      "2022-05-07 06:22:31.336898: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8000 of size 256 next 367\n",
      "2022-05-07 06:22:31.336915: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8100 of size 256 next 474\n",
      "2022-05-07 06:22:31.336931: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8200 of size 256 next 459\n",
      "2022-05-07 06:22:31.336948: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8300 of size 256 next 497\n",
      "2022-05-07 06:22:31.336963: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8400 of size 256 next 455\n",
      "2022-05-07 06:22:31.336981: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8500 of size 256 next 462\n",
      "2022-05-07 06:22:31.336998: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8600 of size 256 next 466\n",
      "2022-05-07 06:22:31.337013: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8700 of size 512 next 493\n",
      "2022-05-07 06:22:31.337030: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8900 of size 768 next 456\n",
      "2022-05-07 06:22:31.337047: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8c00 of size 256 next 458\n",
      "2022-05-07 06:22:31.337064: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8d00 of size 256 next 598\n",
      "2022-05-07 06:22:31.337080: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8e00 of size 256 next 483\n",
      "2022-05-07 06:22:31.337095: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc8f00 of size 256 next 495\n",
      "2022-05-07 06:22:31.337112: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9000 of size 256 next 491\n",
      "2022-05-07 06:22:31.337129: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9100 of size 512 next 496\n",
      "2022-05-07 06:22:31.337146: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9300 of size 512 next 479\n",
      "2022-05-07 06:22:31.337161: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9500 of size 512 next 478\n",
      "2022-05-07 06:22:31.337179: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9700 of size 256 next 578\n",
      "2022-05-07 06:22:31.337195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9800 of size 256 next 579\n",
      "2022-05-07 06:22:31.337211: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9900 of size 256 next 559\n",
      "2022-05-07 06:22:31.337226: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9a00 of size 256 next 499\n",
      "2022-05-07 06:22:31.337244: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fc9b00 of size 2816 next 486\n",
      "2022-05-07 06:22:31.337261: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fca600 of size 2816 next 457\n",
      "2022-05-07 06:22:31.337277: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcb100 of size 256 next 464\n",
      "2022-05-07 06:22:31.337293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcb200 of size 256 next 597\n",
      "2022-05-07 06:22:31.337310: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcb300 of size 256 next 289\n",
      "2022-05-07 06:22:31.337326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcb400 of size 256 next 453\n",
      "2022-05-07 06:22:31.337341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcb500 of size 2816 next 449\n",
      "2022-05-07 06:22:31.337357: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc000 of size 256 next 361\n",
      "2022-05-07 06:22:31.337374: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc100 of size 256 next 475\n",
      "2022-05-07 06:22:31.337391: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc200 of size 256 next 490\n",
      "2022-05-07 06:22:31.337409: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc300 of size 256 next 502\n",
      "2022-05-07 06:22:31.337425: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc400 of size 256 next 503\n",
      "2022-05-07 06:22:31.337442: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc500 of size 256 next 504\n",
      "2022-05-07 06:22:31.337458: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc600 of size 256 next 506\n",
      "2022-05-07 06:22:31.337473: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc700 of size 256 next 507\n",
      "2022-05-07 06:22:31.337489: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc800 of size 256 next 508\n",
      "2022-05-07 06:22:31.337506: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcc900 of size 256 next 509\n",
      "2022-05-07 06:22:31.337522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcca00 of size 256 next 510\n",
      "2022-05-07 06:22:31.337539: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fccb00 of size 256 next 511\n",
      "2022-05-07 06:22:31.337555: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fccc00 of size 512 next 513\n",
      "2022-05-07 06:22:31.337572: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcce00 of size 512 next 514\n",
      "2022-05-07 06:22:31.337589: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcd000 of size 512 next 515\n",
      "2022-05-07 06:22:31.337604: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcd200 of size 2048 next 465\n",
      "2022-05-07 06:22:31.337621: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fcda00 of size 17152 next 476\n",
      "2022-05-07 06:22:31.337639: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd1d00 of size 13824 next 489\n",
      "2022-05-07 06:22:31.337656: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd5300 of size 4096 next 450\n",
      "2022-05-07 06:22:31.337673: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd6300 of size 4096 next 516\n",
      "2022-05-07 06:22:31.337688: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd7300 of size 256 next 518\n",
      "2022-05-07 06:22:31.337705: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd7400 of size 2816 next 519\n",
      "2022-05-07 06:22:31.337722: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd7f00 of size 256 next 520\n",
      "2022-05-07 06:22:31.337738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8000 of size 256 next 521\n",
      "2022-05-07 06:22:31.337753: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8100 of size 256 next 522\n",
      "2022-05-07 06:22:31.337770: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8200 of size 256 next 524\n",
      "2022-05-07 06:22:31.337787: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8300 of size 256 next 525\n",
      "2022-05-07 06:22:31.337803: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8400 of size 256 next 526\n",
      "2022-05-07 06:22:31.337818: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8500 of size 256 next 528\n",
      "2022-05-07 06:22:31.337835: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8600 of size 256 next 529\n",
      "2022-05-07 06:22:31.337852: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8700 of size 256 next 530\n",
      "2022-05-07 06:22:31.337868: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8800 of size 256 next 532\n",
      "2022-05-07 06:22:31.337884: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8900 of size 256 next 533\n",
      "2022-05-07 06:22:31.337901: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8a00 of size 256 next 534\n",
      "2022-05-07 06:22:31.337917: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8b00 of size 512 next 536\n",
      "2022-05-07 06:22:31.337932: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8d00 of size 512 next 537\n",
      "2022-05-07 06:22:31.337948: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd8f00 of size 512 next 538\n",
      "2022-05-07 06:22:31.337965: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd9100 of size 256 next 543\n",
      "2022-05-07 06:22:31.337982: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd9200 of size 256 next 448\n",
      "2022-05-07 06:22:31.337999: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fd9300 of size 16384 next 460\n",
      "2022-05-07 06:22:31.338015: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fdd300 of size 13824 next 501\n",
      "2022-05-07 06:22:31.338032: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fe0900 of size 16384 next 505\n",
      "2022-05-07 06:22:31.338050: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4fe4900 of size 100864 next 463\n",
      "2022-05-07 06:22:31.338066: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada4ffd300 of size 65536 next 477\n",
      "2022-05-07 06:22:31.338082: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada500d300 of size 262144 next 512\n",
      "2022-05-07 06:22:31.338099: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada504d300 of size 10240 next 517\n",
      "2022-05-07 06:22:31.338116: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada504fb00 of size 13824 next 523\n",
      "2022-05-07 06:22:31.338133: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5053100 of size 16384 next 527\n",
      "2022-05-07 06:22:31.338149: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5057100 of size 65536 next 531\n",
      "2022-05-07 06:22:31.338166: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5067100 of size 4096 next 540\n",
      "2022-05-07 06:22:31.338183: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5068100 of size 2048 next 541\n",
      "2022-05-07 06:22:31.338198: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5068900 of size 10240 next 542\n",
      "2022-05-07 06:22:31.338214: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b100 of size 256 next 544\n",
      "2022-05-07 06:22:31.338231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b200 of size 256 next 545\n",
      "2022-05-07 06:22:31.338248: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b300 of size 256 next 546\n",
      "2022-05-07 06:22:31.338264: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b400 of size 256 next 547\n",
      "2022-05-07 06:22:31.338280: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b500 of size 256 next 548\n",
      "2022-05-07 06:22:31.338297: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b600 of size 256 next 549\n",
      "2022-05-07 06:22:31.338314: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b700 of size 256 next 550\n",
      "2022-05-07 06:22:31.338329: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b800 of size 256 next 551\n",
      "2022-05-07 06:22:31.338344: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506b900 of size 256 next 590\n",
      "2022-05-07 06:22:31.338362: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506ba00 of size 256 next 667\n",
      "2022-05-07 06:22:31.338378: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506bb00 of size 256 next 494\n",
      "2022-05-07 06:22:31.338394: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506bc00 of size 256 next 584\n",
      "2022-05-07 06:22:31.338410: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506bd00 of size 256 next 588\n",
      "2022-05-07 06:22:31.338427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506be00 of size 256 next 602\n",
      "2022-05-07 06:22:31.338444: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506bf00 of size 256 next 595\n",
      "2022-05-07 06:22:31.338460: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c000 of size 256 next 560\n",
      "2022-05-07 06:22:31.338475: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c100 of size 256 next 561\n",
      "2022-05-07 06:22:31.338492: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c200 of size 256 next 571\n",
      "2022-05-07 06:22:31.338508: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c300 of size 256 next 574\n",
      "2022-05-07 06:22:31.338524: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c400 of size 256 next 596\n",
      "2022-05-07 06:22:31.338540: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c500 of size 256 next 567\n",
      "2022-05-07 06:22:31.338558: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c600 of size 256 next 688\n",
      "2022-05-07 06:22:31.338575: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c700 of size 256 next 557\n",
      "2022-05-07 06:22:31.338591: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c800 of size 256 next 569\n",
      "2022-05-07 06:22:31.338606: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506c900 of size 256 next 558\n",
      "2022-05-07 06:22:31.338624: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506ca00 of size 512 next 600\n",
      "2022-05-07 06:22:31.338640: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506cc00 of size 512 next 487\n",
      "2022-05-07 06:22:31.338656: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506ce00 of size 256 next 587\n",
      "2022-05-07 06:22:31.338672: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506cf00 of size 256 next 657\n",
      "2022-05-07 06:22:31.338689: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506d000 of size 256 next 582\n",
      "2022-05-07 06:22:31.338706: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506d100 of size 512 next 554\n",
      "2022-05-07 06:22:31.338722: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506d300 of size 512 next 589\n",
      "2022-05-07 06:22:31.338737: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506d500 of size 512 next 565\n",
      "2022-05-07 06:22:31.338755: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506d700 of size 256 next 690\n",
      "2022-05-07 06:22:31.338771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506d800 of size 256 next 679\n",
      "2022-05-07 06:22:31.338787: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506d900 of size 256 next 658\n",
      "2022-05-07 06:22:31.338802: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506da00 of size 256 next 591\n",
      "2022-05-07 06:22:31.338820: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506db00 of size 2304 next 592\n",
      "2022-05-07 06:22:31.338837: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506e400 of size 2816 next 473\n",
      "2022-05-07 06:22:31.338854: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506ef00 of size 256 next 575\n",
      "2022-05-07 06:22:31.338870: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506f000 of size 256 next 702\n",
      "2022-05-07 06:22:31.338887: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506f100 of size 256 next 564\n",
      "2022-05-07 06:22:31.338903: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506f200 of size 256 next 553\n",
      "2022-05-07 06:22:31.338919: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506f300 of size 2816 next 566\n",
      "2022-05-07 06:22:31.338935: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506fe00 of size 256 next 552\n",
      "2022-05-07 06:22:31.338952: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada506ff00 of size 256 next 594\n",
      "2022-05-07 06:22:31.338968: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070000 of size 256 next 603\n",
      "2022-05-07 06:22:31.338984: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070100 of size 256 next 605\n",
      "2022-05-07 06:22:31.339000: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070200 of size 256 next 606\n",
      "2022-05-07 06:22:31.339016: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070300 of size 256 next 607\n",
      "2022-05-07 06:22:31.339033: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070400 of size 256 next 609\n",
      "2022-05-07 06:22:31.339049: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070500 of size 256 next 610\n",
      "2022-05-07 06:22:31.339065: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070600 of size 256 next 611\n",
      "2022-05-07 06:22:31.339082: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070700 of size 256 next 613\n",
      "2022-05-07 06:22:31.339098: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070800 of size 256 next 614\n",
      "2022-05-07 06:22:31.339114: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070900 of size 256 next 615\n",
      "2022-05-07 06:22:31.339129: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070a00 of size 512 next 616\n",
      "2022-05-07 06:22:31.339146: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070c00 of size 512 next 617\n",
      "2022-05-07 06:22:31.339163: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5070e00 of size 512 next 618\n",
      "2022-05-07 06:22:31.339180: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5071000 of size 2048 next 583\n",
      "2022-05-07 06:22:31.339196: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5071800 of size 17152 next 573\n",
      "2022-05-07 06:22:31.339220: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5075b00 of size 13824 next 568\n",
      "2022-05-07 06:22:31.339236: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5079100 of size 4096 next 601\n",
      "2022-05-07 06:22:31.339252: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507a100 of size 4096 next 620\n",
      "2022-05-07 06:22:31.339267: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507b100 of size 256 next 622\n",
      "2022-05-07 06:22:31.339284: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507b200 of size 2816 next 623\n",
      "2022-05-07 06:22:31.339302: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507bd00 of size 256 next 624\n",
      "2022-05-07 06:22:31.339318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507be00 of size 256 next 625\n",
      "2022-05-07 06:22:31.339334: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507bf00 of size 256 next 626\n",
      "2022-05-07 06:22:31.339351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c000 of size 256 next 628\n",
      "2022-05-07 06:22:31.339367: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c100 of size 256 next 629\n",
      "2022-05-07 06:22:31.339383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c200 of size 256 next 630\n",
      "2022-05-07 06:22:31.339398: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c300 of size 256 next 632\n",
      "2022-05-07 06:22:31.339415: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c400 of size 256 next 633\n",
      "2022-05-07 06:22:31.339432: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c500 of size 256 next 634\n",
      "2022-05-07 06:22:31.339449: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c600 of size 256 next 636\n",
      "2022-05-07 06:22:31.339464: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c700 of size 256 next 637\n",
      "2022-05-07 06:22:31.339481: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c800 of size 256 next 638\n",
      "2022-05-07 06:22:31.339498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507c900 of size 512 next 639\n",
      "2022-05-07 06:22:31.339527: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507cb00 of size 512 next 640\n",
      "2022-05-07 06:22:31.339544: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507cd00 of size 512 next 641\n",
      "2022-05-07 06:22:31.339561: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507cf00 of size 256 next 645\n",
      "2022-05-07 06:22:31.339577: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507d000 of size 256 next 576\n",
      "2022-05-07 06:22:31.339593: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada507d100 of size 16384 next 572\n",
      "2022-05-07 06:22:31.339610: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5081100 of size 13824 next 604\n",
      "2022-05-07 06:22:31.339627: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5084700 of size 16384 next 608\n",
      "2022-05-07 06:22:31.339645: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5088700 of size 19456 next 500\n",
      "2022-05-07 06:22:31.339661: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada508d300 of size 262144 next 470\n",
      "2022-05-07 06:22:31.339678: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada50cd300 of size 262144 next 535\n",
      "2022-05-07 06:22:31.339695: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada510d300 of size 65536 next 612\n",
      "2022-05-07 06:22:31.339712: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada511d300 of size 13824 next 627\n",
      "2022-05-07 06:22:31.339727: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5120900 of size 16384 next 631\n",
      "2022-05-07 06:22:31.339744: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5124900 of size 4096 next 642\n",
      "2022-05-07 06:22:31.339761: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5125900 of size 2048 next 643\n",
      "2022-05-07 06:22:31.339778: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5126100 of size 10240 next 644\n",
      "2022-05-07 06:22:31.339795: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5128900 of size 256 next 646\n",
      "2022-05-07 06:22:31.339811: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5128a00 of size 256 next 647\n",
      "2022-05-07 06:22:31.339828: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5128b00 of size 256 next 648\n",
      "2022-05-07 06:22:31.339844: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5128c00 of size 256 next 649\n",
      "2022-05-07 06:22:31.339859: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5128d00 of size 256 next 650\n",
      "2022-05-07 06:22:31.339875: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5128e00 of size 256 next 651\n",
      "2022-05-07 06:22:31.339892: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5128f00 of size 256 next 652\n",
      "2022-05-07 06:22:31.339909: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129000 of size 256 next 653\n",
      "2022-05-07 06:22:31.339925: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129100 of size 256 next 699\n",
      "2022-05-07 06:22:31.339941: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129200 of size 256 next 661\n",
      "2022-05-07 06:22:31.339958: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129300 of size 256 next 678\n",
      "2022-05-07 06:22:31.339974: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129400 of size 256 next 779\n",
      "2022-05-07 06:22:31.339990: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129500 of size 256 next 669\n",
      "2022-05-07 06:22:31.340006: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129600 of size 256 next 684\n",
      "2022-05-07 06:22:31.340023: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129700 of size 256 next 680\n",
      "2022-05-07 06:22:31.340039: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129800 of size 256 next 659\n",
      "2022-05-07 06:22:31.340056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129900 of size 256 next 696\n",
      "2022-05-07 06:22:31.340072: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129a00 of size 256 next 707\n",
      "2022-05-07 06:22:31.340089: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129b00 of size 256 next 682\n",
      "2022-05-07 06:22:31.340105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129c00 of size 256 next 670\n",
      "2022-05-07 06:22:31.340121: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129d00 of size 256 next 703\n",
      "2022-05-07 06:22:31.340137: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129e00 of size 256 next 783\n",
      "2022-05-07 06:22:31.340153: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5129f00 of size 256 next 655\n",
      "2022-05-07 06:22:31.340170: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512a000 of size 256 next 698\n",
      "2022-05-07 06:22:31.340186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512a100 of size 256 next 683\n",
      "2022-05-07 06:22:31.340202: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512a200 of size 256 next 656\n",
      "2022-05-07 06:22:31.340219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512a300 of size 512 next 673\n",
      "2022-05-07 06:22:31.340235: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512a500 of size 768 next 675\n",
      "2022-05-07 06:22:31.340252: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512a800 of size 512 next 556\n",
      "2022-05-07 06:22:31.340268: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512aa00 of size 512 next 681\n",
      "2022-05-07 06:22:31.340285: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512ac00 of size 512 next 705\n",
      "2022-05-07 06:22:31.340302: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512ae00 of size 512 next 689\n",
      "2022-05-07 06:22:31.340317: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7fada512b000 of size 1024 next 674\n",
      "2022-05-07 06:22:31.340333: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512b400 of size 256 next 555\n",
      "2022-05-07 06:22:31.340350: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512b500 of size 256 next 562\n",
      "2022-05-07 06:22:31.340367: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512b600 of size 256 next 676\n",
      "2022-05-07 06:22:31.340383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512b700 of size 2816 next 704\n",
      "2022-05-07 06:22:31.340400: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512c200 of size 4352 next 563\n",
      "2022-05-07 06:22:31.340417: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada512d300 of size 65536 next 498\n",
      "2022-05-07 06:22:31.340433: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada513d300 of size 65536 next 635\n",
      "2022-05-07 06:22:31.340450: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada514d300 of size 383488 next 376\n",
      "2022-05-07 06:22:31.340466: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada51aad00 of size 2000128 next 370\n",
      "2022-05-07 06:22:31.340483: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada5393200 of size 2000128 next 435\n",
      "2022-05-07 06:22:31.340501: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada557b700 of size 2562304 next 151\n",
      "2022-05-07 06:22:31.340519: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada57ed000 of size 25088000 next 176\n",
      "2022-05-07 06:22:31.340536: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada6fda000 of size 25088000 next 226\n",
      "2022-05-07 06:22:31.340553: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada87c7000 of size 25088000 next 308\n",
      "2022-05-07 06:22:31.340568: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fada9fb4000 of size 25088000 next 262\n",
      "2022-05-07 06:22:31.340584: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadab7a1000 of size 25088000 next 259\n",
      "2022-05-07 06:22:31.340601: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadacf8e000 of size 41472000 next 412\n",
      "2022-05-07 06:22:31.340619: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadaf71b000 of size 76435456 next 18446744073709551615\n",
      "2022-05-07 06:22:31.340637: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 268435456\n",
      "2022-05-07 06:22:31.340654: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadcc000000 of size 41472000 next 372\n",
      "2022-05-07 06:22:31.340671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadce78d000 of size 2000128 next 452\n",
      "2022-05-07 06:22:31.340688: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadce975500 of size 2000128 next 461\n",
      "2022-05-07 06:22:31.340705: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadceb5da00 of size 78943744 next 492\n",
      "2022-05-07 06:22:31.340721: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd36a7000 of size 41472000 next 451\n",
      "2022-05-07 06:22:31.340737: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd5e34000 of size 41472000 next 539\n",
      "2022-05-07 06:22:31.340755: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd85c1000 of size 262144 next 581\n",
      "2022-05-07 06:22:31.340772: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd8601000 of size 262144 next 585\n",
      "2022-05-07 06:22:31.340788: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd8641000 of size 2000128 next 621\n",
      "2022-05-07 06:22:31.340804: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd8829500 of size 2000128 next 580\n",
      "2022-05-07 06:22:31.340821: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd8a11a00 of size 2000128 next 599\n",
      "2022-05-07 06:22:31.340838: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadd8bf9f00 of size 25088000 next 619\n",
      "2022-05-07 06:22:31.340855: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fadda3e6f00 of size 29462784 next 18446744073709551615\n",
      "2022-05-07 06:22:31.340871: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 67108864\n",
      "2022-05-07 06:22:31.340889: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae0e000000 of size 25088000 next 54\n",
      "2022-05-07 06:22:31.340906: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae0f7ed000 of size 262144 next 167\n",
      "2022-05-07 06:22:31.340922: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae0f82d000 of size 262144 next 174\n",
      "2022-05-07 06:22:31.340939: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae0f86d000 of size 41496576 next 18446744073709551615\n",
      "2022-05-07 06:22:31.340956: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 33554432\n",
      "2022-05-07 06:22:31.340973: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae12000000 of size 2000128 next 92\n",
      "2022-05-07 06:22:31.340990: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae121e8500 of size 2000128 next 60\n",
      "2022-05-07 06:22:31.341006: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae123d0a00 of size 2000128 next 59\n",
      "2022-05-07 06:22:31.341024: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae125b8f00 of size 27554048 next 18446744073709551615\n",
      "2022-05-07 06:22:31.341041: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 33554432\n",
      "2022-05-07 06:22:31.341058: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae1a000000 of size 33554432 next 18446744073709551615\n",
      "2022-05-07 06:22:31.341075: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2097152\n",
      "2022-05-07 06:22:31.341093: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400000 of size 256 next 1\n",
      "2022-05-07 06:22:31.341110: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400100 of size 1280 next 2\n",
      "2022-05-07 06:22:31.341127: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400600 of size 256 next 3\n",
      "2022-05-07 06:22:31.341143: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400700 of size 256 next 4\n",
      "2022-05-07 06:22:31.341159: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400800 of size 256 next 5\n",
      "2022-05-07 06:22:31.341175: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400900 of size 256 next 6\n",
      "2022-05-07 06:22:31.341191: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400a00 of size 256 next 9\n",
      "2022-05-07 06:22:31.341207: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400b00 of size 256 next 10\n",
      "2022-05-07 06:22:31.341223: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400c00 of size 256 next 11\n",
      "2022-05-07 06:22:31.341240: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400d00 of size 256 next 153\n",
      "2022-05-07 06:22:31.341256: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400e00 of size 256 next 13\n",
      "2022-05-07 06:22:31.341273: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42400f00 of size 256 next 14\n",
      "2022-05-07 06:22:31.341289: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401000 of size 256 next 15\n",
      "2022-05-07 06:22:31.341305: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401100 of size 256 next 16\n",
      "2022-05-07 06:22:31.341320: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401200 of size 256 next 19\n",
      "2022-05-07 06:22:31.341336: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401300 of size 256 next 20\n",
      "2022-05-07 06:22:31.341353: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401400 of size 256 next 146\n",
      "2022-05-07 06:22:31.341370: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401500 of size 256 next 22\n",
      "2022-05-07 06:22:31.341386: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401600 of size 256 next 23\n",
      "2022-05-07 06:22:31.341402: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401700 of size 256 next 24\n",
      "2022-05-07 06:22:31.341419: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401800 of size 256 next 27\n",
      "2022-05-07 06:22:31.341435: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401900 of size 256 next 28\n",
      "2022-05-07 06:22:31.341450: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401a00 of size 256 next 29\n",
      "2022-05-07 06:22:31.341466: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401b00 of size 256 next 185\n",
      "2022-05-07 06:22:31.341483: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401c00 of size 256 next 31\n",
      "2022-05-07 06:22:31.341500: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401d00 of size 256 next 32\n",
      "2022-05-07 06:22:31.341518: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401e00 of size 256 next 7\n",
      "2022-05-07 06:22:31.341534: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42401f00 of size 2816 next 8\n",
      "2022-05-07 06:22:31.341551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42402a00 of size 2816 next 75\n",
      "2022-05-07 06:22:31.341568: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42403500 of size 24832 next 18\n",
      "2022-05-07 06:22:31.341585: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42409600 of size 13824 next 17\n",
      "2022-05-07 06:22:31.341602: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240cc00 of size 256 next 33\n",
      "2022-05-07 06:22:31.341619: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240cd00 of size 256 next 36\n",
      "2022-05-07 06:22:31.341635: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240ce00 of size 256 next 37\n",
      "2022-05-07 06:22:31.341651: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240cf00 of size 256 next 272\n",
      "2022-05-07 06:22:31.341667: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240d000 of size 256 next 39\n",
      "2022-05-07 06:22:31.341684: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240d100 of size 256 next 40\n",
      "2022-05-07 06:22:31.341701: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240d200 of size 256 next 41\n",
      "2022-05-07 06:22:31.341717: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240d300 of size 512 next 42\n",
      "2022-05-07 06:22:31.341733: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240d500 of size 512 next 45\n",
      "2022-05-07 06:22:31.341750: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240d700 of size 512 next 46\n",
      "2022-05-07 06:22:31.341766: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240d900 of size 256 next 141\n",
      "2022-05-07 06:22:31.341782: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240da00 of size 256 next 160\n",
      "2022-05-07 06:22:31.341798: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240db00 of size 256 next 152\n",
      "2022-05-07 06:22:31.341815: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240dc00 of size 256 next 48\n",
      "2022-05-07 06:22:31.341832: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240dd00 of size 256 next 49\n",
      "2022-05-07 06:22:31.341848: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240de00 of size 256 next 50\n",
      "2022-05-07 06:22:31.341864: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240df00 of size 4096 next 55\n",
      "2022-05-07 06:22:31.341881: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240ef00 of size 256 next 56\n",
      "2022-05-07 06:22:31.341897: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240f000 of size 256 next 57\n",
      "2022-05-07 06:22:31.341913: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240f100 of size 2048 next 58\n",
      "2022-05-07 06:22:31.341929: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240f900 of size 256 next 61\n",
      "2022-05-07 06:22:31.341946: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240fa00 of size 256 next 62\n",
      "2022-05-07 06:22:31.341963: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240fb00 of size 256 next 63\n",
      "2022-05-07 06:22:31.341979: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240fc00 of size 256 next 161\n",
      "2022-05-07 06:22:31.341995: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240fd00 of size 256 next 66\n",
      "2022-05-07 06:22:31.342012: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240fe00 of size 256 next 67\n",
      "2022-05-07 06:22:31.342028: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4240ff00 of size 256 next 69\n",
      "2022-05-07 06:22:31.342043: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410000 of size 256 next 70\n",
      "2022-05-07 06:22:31.342059: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410100 of size 256 next 71\n",
      "2022-05-07 06:22:31.342076: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410200 of size 256 next 72\n",
      "2022-05-07 06:22:31.342096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410300 of size 256 next 73\n",
      "2022-05-07 06:22:31.342113: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410400 of size 256 next 74\n",
      "2022-05-07 06:22:31.342128: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410500 of size 256 next 76\n",
      "2022-05-07 06:22:31.342145: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410600 of size 256 next 77\n",
      "2022-05-07 06:22:31.342162: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410700 of size 256 next 78\n",
      "2022-05-07 06:22:31.342177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410800 of size 256 next 79\n",
      "2022-05-07 06:22:31.342193: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410900 of size 256 next 80\n",
      "2022-05-07 06:22:31.342210: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410a00 of size 256 next 81\n",
      "2022-05-07 06:22:31.342226: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410b00 of size 256 next 26\n",
      "2022-05-07 06:22:31.342243: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42410c00 of size 16384 next 25\n",
      "2022-05-07 06:22:31.342260: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42414c00 of size 10240 next 64\n",
      "2022-05-07 06:22:31.342276: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42417400 of size 16384 next 82\n",
      "2022-05-07 06:22:31.342293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4241b400 of size 256 next 83\n",
      "2022-05-07 06:22:31.342308: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4241b500 of size 256 next 84\n",
      "2022-05-07 06:22:31.342325: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4241b600 of size 103936 next 35\n",
      "2022-05-07 06:22:31.342343: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42434c00 of size 65536 next 34\n",
      "2022-05-07 06:22:31.342359: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42444c00 of size 256 next 85\n",
      "2022-05-07 06:22:31.342375: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42444d00 of size 256 next 86\n",
      "2022-05-07 06:22:31.342391: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42444e00 of size 256 next 87\n",
      "2022-05-07 06:22:31.342409: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42444f00 of size 523520 next 44\n",
      "2022-05-07 06:22:31.342427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae424c4c00 of size 262144 next 43\n",
      "2022-05-07 06:22:31.342442: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42504c00 of size 512 next 88\n",
      "2022-05-07 06:22:31.342459: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42504e00 of size 512 next 89\n",
      "2022-05-07 06:22:31.342476: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42505000 of size 512 next 90\n",
      "2022-05-07 06:22:31.342492: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42505200 of size 4096 next 91\n",
      "2022-05-07 06:22:31.342508: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42506200 of size 2048 next 93\n",
      "2022-05-07 06:22:31.342525: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42506a00 of size 10240 next 94\n",
      "2022-05-07 06:22:31.342542: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42509200 of size 256 next 95\n",
      "2022-05-07 06:22:31.342559: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42509300 of size 2816 next 96\n",
      "2022-05-07 06:22:31.342574: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42509e00 of size 256 next 97\n",
      "2022-05-07 06:22:31.342590: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42509f00 of size 256 next 98\n",
      "2022-05-07 06:22:31.342607: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4250a000 of size 256 next 99\n",
      "2022-05-07 06:22:31.342624: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4250a100 of size 13824 next 100\n",
      "2022-05-07 06:22:31.342640: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4250d700 of size 256 next 101\n",
      "2022-05-07 06:22:31.342656: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4250d800 of size 256 next 102\n",
      "2022-05-07 06:22:31.342673: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4250d900 of size 256 next 103\n",
      "2022-05-07 06:22:31.342690: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4250da00 of size 16384 next 104\n",
      "2022-05-07 06:22:31.342705: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42511a00 of size 256 next 105\n",
      "2022-05-07 06:22:31.342721: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42511b00 of size 256 next 106\n",
      "2022-05-07 06:22:31.342738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42511c00 of size 256 next 107\n",
      "2022-05-07 06:22:31.342755: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42511d00 of size 65536 next 108\n",
      "2022-05-07 06:22:31.342771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42521d00 of size 256 next 109\n",
      "2022-05-07 06:22:31.342787: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42521e00 of size 256 next 110\n",
      "2022-05-07 06:22:31.342804: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42521f00 of size 256 next 111\n",
      "2022-05-07 06:22:31.342820: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42522000 of size 262144 next 112\n",
      "2022-05-07 06:22:31.342836: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42562000 of size 512 next 113\n",
      "2022-05-07 06:22:31.342852: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42562200 of size 512 next 114\n",
      "2022-05-07 06:22:31.342869: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42562400 of size 512 next 115\n",
      "2022-05-07 06:22:31.342886: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42562600 of size 4096 next 116\n",
      "2022-05-07 06:22:31.342903: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42563600 of size 2048 next 117\n",
      "2022-05-07 06:22:31.342919: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42563e00 of size 10240 next 118\n",
      "2022-05-07 06:22:31.342936: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566600 of size 256 next 119\n",
      "2022-05-07 06:22:31.342952: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566700 of size 256 next 120\n",
      "2022-05-07 06:22:31.342968: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566800 of size 256 next 121\n",
      "2022-05-07 06:22:31.342984: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566900 of size 256 next 122\n",
      "2022-05-07 06:22:31.343000: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566a00 of size 256 next 123\n",
      "2022-05-07 06:22:31.343017: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566b00 of size 256 next 124\n",
      "2022-05-07 06:22:31.343033: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566c00 of size 256 next 125\n",
      "2022-05-07 06:22:31.343049: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566d00 of size 256 next 126\n",
      "2022-05-07 06:22:31.343066: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566e00 of size 256 next 127\n",
      "2022-05-07 06:22:31.343083: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42566f00 of size 256 next 128\n",
      "2022-05-07 06:22:31.343099: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567000 of size 256 next 133\n",
      "2022-05-07 06:22:31.343115: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567100 of size 256 next 265\n",
      "2022-05-07 06:22:31.343132: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567200 of size 256 next 183\n",
      "2022-05-07 06:22:31.343148: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567300 of size 256 next 132\n",
      "2022-05-07 06:22:31.343165: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567400 of size 256 next 162\n",
      "2022-05-07 06:22:31.343182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567500 of size 256 next 137\n",
      "2022-05-07 06:22:31.343199: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567600 of size 256 next 138\n",
      "2022-05-07 06:22:31.343215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567700 of size 256 next 168\n",
      "2022-05-07 06:22:31.343230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567800 of size 256 next 173\n",
      "2022-05-07 06:22:31.343246: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567900 of size 256 next 134\n",
      "2022-05-07 06:22:31.343263: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567a00 of size 256 next 175\n",
      "2022-05-07 06:22:31.343280: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567b00 of size 256 next 140\n",
      "2022-05-07 06:22:31.343296: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567c00 of size 256 next 179\n",
      "2022-05-07 06:22:31.343312: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567d00 of size 256 next 169\n",
      "2022-05-07 06:22:31.343329: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567e00 of size 256 next 274\n",
      "2022-05-07 06:22:31.343345: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42567f00 of size 256 next 130\n",
      "2022-05-07 06:22:31.343361: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568000 of size 512 next 170\n",
      "2022-05-07 06:22:31.343377: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568200 of size 512 next 142\n",
      "2022-05-07 06:22:31.343394: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568400 of size 256 next 172\n",
      "2022-05-07 06:22:31.343410: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568500 of size 256 next 163\n",
      "2022-05-07 06:22:31.343427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568600 of size 256 next 287\n",
      "2022-05-07 06:22:31.343443: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568700 of size 256 next 159\n",
      "2022-05-07 06:22:31.343460: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568800 of size 512 next 38\n",
      "2022-05-07 06:22:31.343476: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568a00 of size 512 next 181\n",
      "2022-05-07 06:22:31.343492: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568c00 of size 512 next 144\n",
      "2022-05-07 06:22:31.343519: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568e00 of size 256 next 246\n",
      "2022-05-07 06:22:31.343536: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42568f00 of size 256 next 260\n",
      "2022-05-07 06:22:31.343552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42569000 of size 256 next 261\n",
      "2022-05-07 06:22:31.343568: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42569100 of size 256 next 143\n",
      "2022-05-07 06:22:31.343587: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42569200 of size 3584 next 129\n",
      "2022-05-07 06:22:31.343603: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256a000 of size 2816 next 148\n",
      "2022-05-07 06:22:31.343619: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256ab00 of size 256 next 47\n",
      "2022-05-07 06:22:31.343635: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256ac00 of size 256 next 269\n",
      "2022-05-07 06:22:31.343652: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256ad00 of size 256 next 147\n",
      "2022-05-07 06:22:31.343669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256ae00 of size 256 next 182\n",
      "2022-05-07 06:22:31.343684: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256af00 of size 2816 next 171\n",
      "2022-05-07 06:22:31.343700: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256ba00 of size 256 next 131\n",
      "2022-05-07 06:22:31.343717: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256bb00 of size 256 next 186\n",
      "2022-05-07 06:22:31.343734: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256bc00 of size 256 next 187\n",
      "2022-05-07 06:22:31.343750: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256bd00 of size 256 next 189\n",
      "2022-05-07 06:22:31.343766: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256be00 of size 256 next 190\n",
      "2022-05-07 06:22:31.343783: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256bf00 of size 256 next 191\n",
      "2022-05-07 06:22:31.343799: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c000 of size 256 next 193\n",
      "2022-05-07 06:22:31.343814: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c100 of size 256 next 194\n",
      "2022-05-07 06:22:31.343830: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c200 of size 256 next 195\n",
      "2022-05-07 06:22:31.343847: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c300 of size 256 next 196\n",
      "2022-05-07 06:22:31.343864: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c400 of size 256 next 197\n",
      "2022-05-07 06:22:31.343880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c500 of size 256 next 198\n",
      "2022-05-07 06:22:31.343896: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c600 of size 512 next 199\n",
      "2022-05-07 06:22:31.343915: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256c800 of size 512 next 200\n",
      "2022-05-07 06:22:31.343931: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256ca00 of size 512 next 201\n",
      "2022-05-07 06:22:31.343947: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256cc00 of size 2048 next 21\n",
      "2022-05-07 06:22:31.343962: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4256d400 of size 17152 next 150\n",
      "2022-05-07 06:22:31.343980: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42571700 of size 13824 next 178\n",
      "2022-05-07 06:22:31.344008: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42574d00 of size 4096 next 165\n",
      "2022-05-07 06:22:31.344014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42575d00 of size 4096 next 202\n",
      "2022-05-07 06:22:31.344020: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42576d00 of size 256 next 205\n",
      "2022-05-07 06:22:31.344026: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42576e00 of size 2816 next 206\n",
      "2022-05-07 06:22:31.344033: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42577900 of size 256 next 207\n",
      "2022-05-07 06:22:31.344039: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42577a00 of size 256 next 208\n",
      "2022-05-07 06:22:31.344045: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42577b00 of size 256 next 209\n",
      "2022-05-07 06:22:31.344051: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42577c00 of size 256 next 211\n",
      "2022-05-07 06:22:31.344057: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42577d00 of size 256 next 212\n",
      "2022-05-07 06:22:31.344064: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42577e00 of size 256 next 213\n",
      "2022-05-07 06:22:31.344070: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42577f00 of size 256 next 215\n",
      "2022-05-07 06:22:31.344076: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578000 of size 256 next 216\n",
      "2022-05-07 06:22:31.344083: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578100 of size 256 next 217\n",
      "2022-05-07 06:22:31.344088: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578200 of size 256 next 219\n",
      "2022-05-07 06:22:31.344094: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578300 of size 256 next 220\n",
      "2022-05-07 06:22:31.344101: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578400 of size 256 next 221\n",
      "2022-05-07 06:22:31.344107: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578500 of size 512 next 223\n",
      "2022-05-07 06:22:31.344113: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578700 of size 512 next 224\n",
      "2022-05-07 06:22:31.344119: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578900 of size 512 next 225\n",
      "2022-05-07 06:22:31.344126: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578b00 of size 256 next 231\n",
      "2022-05-07 06:22:31.344132: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578c00 of size 256 next 177\n",
      "2022-05-07 06:22:31.344138: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42578d00 of size 16384 next 180\n",
      "2022-05-07 06:22:31.344144: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4257cd00 of size 13824 next 188\n",
      "2022-05-07 06:22:31.344151: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42580300 of size 16384 next 192\n",
      "2022-05-07 06:22:31.344157: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae42584300 of size 100864 next 156\n",
      "2022-05-07 06:22:31.344163: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae4259cd00 of size 65536 next 149\n",
      "2022-05-07 06:22:31.344169: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425acd00 of size 10240 next 204\n",
      "2022-05-07 06:22:31.344176: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425af500 of size 13824 next 210\n",
      "2022-05-07 06:22:31.344182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425b2b00 of size 16384 next 214\n",
      "2022-05-07 06:22:31.344189: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425b6b00 of size 65536 next 218\n",
      "2022-05-07 06:22:31.344195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425c6b00 of size 4096 next 227\n",
      "2022-05-07 06:22:31.344202: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425c7b00 of size 2048 next 229\n",
      "2022-05-07 06:22:31.344208: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425c8300 of size 10240 next 230\n",
      "2022-05-07 06:22:31.344214: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cab00 of size 256 next 232\n",
      "2022-05-07 06:22:31.344220: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cac00 of size 256 next 233\n",
      "2022-05-07 06:22:31.344227: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cad00 of size 256 next 234\n",
      "2022-05-07 06:22:31.344233: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cae00 of size 256 next 235\n",
      "2022-05-07 06:22:31.344239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425caf00 of size 256 next 236\n",
      "2022-05-07 06:22:31.344245: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb000 of size 256 next 237\n",
      "2022-05-07 06:22:31.344252: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb100 of size 256 next 238\n",
      "2022-05-07 06:22:31.344258: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb200 of size 256 next 239\n",
      "2022-05-07 06:22:31.344264: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb300 of size 256 next 356\n",
      "2022-05-07 06:22:31.344270: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb400 of size 256 next 240\n",
      "2022-05-07 06:22:31.344277: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb500 of size 256 next 255\n",
      "2022-05-07 06:22:31.344283: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb600 of size 256 next 264\n",
      "2022-05-07 06:22:31.344289: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb700 of size 256 next 283\n",
      "2022-05-07 06:22:31.344295: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb800 of size 256 next 157\n",
      "2022-05-07 06:22:31.344302: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cb900 of size 256 next 266\n",
      "2022-05-07 06:22:31.344308: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cba00 of size 256 next 244\n",
      "2022-05-07 06:22:31.344314: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cbb00 of size 256 next 279\n",
      "2022-05-07 06:22:31.344320: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cbc00 of size 256 next 166\n",
      "2022-05-07 06:22:31.344327: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cbd00 of size 256 next 256\n",
      "2022-05-07 06:22:31.344333: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cbe00 of size 256 next 245\n",
      "2022-05-07 06:22:31.344339: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cbf00 of size 256 next 275\n",
      "2022-05-07 06:22:31.344345: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc000 of size 256 next 258\n",
      "2022-05-07 06:22:31.344352: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc100 of size 256 next 285\n",
      "2022-05-07 06:22:31.344358: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc200 of size 256 next 277\n",
      "2022-05-07 06:22:31.344364: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc300 of size 256 next 286\n",
      "2022-05-07 06:22:31.344370: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc400 of size 256 next 267\n",
      "2022-05-07 06:22:31.344377: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc500 of size 512 next 251\n",
      "2022-05-07 06:22:31.344383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc700 of size 512 next 65\n",
      "2022-05-07 06:22:31.344389: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cc900 of size 256 next 394\n",
      "2022-05-07 06:22:31.344395: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cca00 of size 256 next 288\n",
      "2022-05-07 06:22:31.344401: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425ccb00 of size 512 next 254\n",
      "2022-05-07 06:22:31.344408: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425ccd00 of size 512 next 248\n",
      "2022-05-07 06:22:31.344414: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425ccf00 of size 512 next 271\n",
      "2022-05-07 06:22:31.344420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cd100 of size 256 next 383\n",
      "2022-05-07 06:22:31.344427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cd200 of size 256 next 391\n",
      "2022-05-07 06:22:31.344433: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cd300 of size 256 next 375\n",
      "2022-05-07 06:22:31.344439: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cd400 of size 256 next 263\n",
      "2022-05-07 06:22:31.344445: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cd500 of size 2560 next 252\n",
      "2022-05-07 06:22:31.344452: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cdf00 of size 2816 next 247\n",
      "2022-05-07 06:22:31.344458: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cea00 of size 256 next 242\n",
      "2022-05-07 06:22:31.344464: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425ceb00 of size 256 next 387\n",
      "2022-05-07 06:22:31.344470: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cec00 of size 256 next 280\n",
      "2022-05-07 06:22:31.344477: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425ced00 of size 256 next 241\n",
      "2022-05-07 06:22:31.344483: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cee00 of size 2816 next 158\n",
      "2022-05-07 06:22:31.344489: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cf900 of size 256 next 290\n",
      "2022-05-07 06:22:31.344495: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cfa00 of size 256 next 291\n",
      "2022-05-07 06:22:31.344502: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cfb00 of size 256 next 292\n",
      "2022-05-07 06:22:31.344508: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cfc00 of size 256 next 294\n",
      "2022-05-07 06:22:31.344514: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cfd00 of size 256 next 295\n",
      "2022-05-07 06:22:31.344520: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cfe00 of size 256 next 296\n",
      "2022-05-07 06:22:31.344526: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425cff00 of size 256 next 298\n",
      "2022-05-07 06:22:31.344533: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0000 of size 256 next 299\n",
      "2022-05-07 06:22:31.344539: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0100 of size 256 next 300\n",
      "2022-05-07 06:22:31.344545: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0200 of size 256 next 301\n",
      "2022-05-07 06:22:31.344551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0300 of size 256 next 302\n",
      "2022-05-07 06:22:31.344558: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0400 of size 256 next 303\n",
      "2022-05-07 06:22:31.344564: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0500 of size 512 next 305\n",
      "2022-05-07 06:22:31.344570: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0700 of size 512 next 306\n",
      "2022-05-07 06:22:31.344576: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0900 of size 512 next 307\n",
      "2022-05-07 06:22:31.344583: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d0b00 of size 2048 next 257\n",
      "2022-05-07 06:22:31.344589: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d1300 of size 17152 next 282\n",
      "2022-05-07 06:22:31.344595: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d5600 of size 13824 next 278\n",
      "2022-05-07 06:22:31.344602: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d8c00 of size 4096 next 268\n",
      "2022-05-07 06:22:31.344608: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425d9c00 of size 4096 next 309\n",
      "2022-05-07 06:22:31.344614: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dac00 of size 256 next 312\n",
      "2022-05-07 06:22:31.344620: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dad00 of size 2816 next 313\n",
      "2022-05-07 06:22:31.344627: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425db800 of size 256 next 314\n",
      "2022-05-07 06:22:31.344633: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425db900 of size 256 next 315\n",
      "2022-05-07 06:22:31.344639: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dba00 of size 256 next 316\n",
      "2022-05-07 06:22:31.344645: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dbb00 of size 256 next 317\n",
      "2022-05-07 06:22:31.344652: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dbc00 of size 256 next 318\n",
      "2022-05-07 06:22:31.344658: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dbd00 of size 256 next 319\n",
      "2022-05-07 06:22:31.344664: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dbe00 of size 256 next 321\n",
      "2022-05-07 06:22:31.344669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dbf00 of size 256 next 322\n",
      "2022-05-07 06:22:31.344676: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dc000 of size 256 next 323\n",
      "2022-05-07 06:22:31.344682: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dc100 of size 256 next 325\n",
      "2022-05-07 06:22:31.344689: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dc200 of size 256 next 326\n",
      "2022-05-07 06:22:31.344695: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dc300 of size 256 next 327\n",
      "2022-05-07 06:22:31.344702: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dc400 of size 512 next 329\n",
      "2022-05-07 06:22:31.344708: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dc600 of size 512 next 330\n",
      "2022-05-07 06:22:31.344714: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dc800 of size 512 next 331\n",
      "2022-05-07 06:22:31.344719: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dca00 of size 256 next 335\n",
      "2022-05-07 06:22:31.344726: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dcb00 of size 256 next 273\n",
      "2022-05-07 06:22:31.344732: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425dcc00 of size 16384 next 270\n",
      "2022-05-07 06:22:31.344739: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7fae425e0c00 of size 128000 next 18446744073709551615\n",
      "2022-05-07 06:22:31.344745: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2022-05-07 06:22:31.344755: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 473 Chunks of size 256 totalling 118.2KiB\n",
      "2022-05-07 06:22:31.344763: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 75 Chunks of size 512 totalling 37.5KiB\n",
      "2022-05-07 06:22:31.344770: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 768 totalling 1.5KiB\n",
      "2022-05-07 06:22:31.344777: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-05-07 06:22:31.344784: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 15 Chunks of size 2048 totalling 30.0KiB\n",
      "2022-05-07 06:22:31.344792: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2304 totalling 2.2KiB\n",
      "2022-05-07 06:22:31.344799: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2560 totalling 5.0KiB\n",
      "2022-05-07 06:22:31.344806: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 22 Chunks of size 2816 totalling 60.5KiB\n",
      "2022-05-07 06:22:31.344813: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB\n",
      "2022-05-07 06:22:31.344821: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3584 totalling 3.5KiB\n",
      "2022-05-07 06:22:31.344828: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 20 Chunks of size 4096 totalling 80.0KiB\n",
      "2022-05-07 06:22:31.344834: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4352 totalling 4.2KiB\n",
      "2022-05-07 06:22:31.344842: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 14 Chunks of size 10240 totalling 140.0KiB\n",
      "2022-05-07 06:22:31.344850: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 17 Chunks of size 13824 totalling 229.5KiB\n",
      "2022-05-07 06:22:31.344857: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 14336 totalling 14.0KiB\n",
      "2022-05-07 06:22:31.344864: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 21 Chunks of size 16384 totalling 336.0KiB\n",
      "2022-05-07 06:22:31.344872: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 17152 totalling 67.0KiB\n",
      "2022-05-07 06:22:31.344879: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 17408 totalling 34.0KiB\n",
      "2022-05-07 06:22:31.344887: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 19456 totalling 19.0KiB\n",
      "2022-05-07 06:22:31.344894: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 24832 totalling 24.2KiB\n",
      "2022-05-07 06:22:31.344901: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 25088 totalling 49.0KiB\n",
      "2022-05-07 06:22:31.344909: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 15 Chunks of size 65536 totalling 960.0KiB\n",
      "2022-05-07 06:22:31.344916: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 74240 totalling 72.5KiB\n",
      "2022-05-07 06:22:31.344923: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 100864 totalling 197.0KiB\n",
      "2022-05-07 06:22:31.344931: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 103936 totalling 101.5KiB\n",
      "2022-05-07 06:22:31.344939: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 114688 totalling 112.0KiB\n",
      "2022-05-07 06:22:31.344946: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 128000 totalling 125.0KiB\n",
      "2022-05-07 06:22:31.344953: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 18 Chunks of size 262144 totalling 4.50MiB\n",
      "2022-05-07 06:22:31.344961: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 345344 totalling 337.2KiB\n",
      "2022-05-07 06:22:31.344968: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 383488 totalling 374.5KiB\n",
      "2022-05-07 06:22:31.344976: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 523520 totalling 511.2KiB\n",
      "2022-05-07 06:22:31.344983: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 20 Chunks of size 2000128 totalling 38.15MiB\n",
      "2022-05-07 06:22:31.344991: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2562304 totalling 2.44MiB\n",
      "2022-05-07 06:22:31.344998: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 8 Chunks of size 25088000 totalling 191.41MiB\n",
      "2022-05-07 06:22:31.345006: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 27554048 totalling 26.28MiB\n",
      "2022-05-07 06:22:31.345013: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 29462784 totalling 28.10MiB\n",
      "2022-05-07 06:22:31.345020: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 33554432 totalling 32.00MiB\n",
      "2022-05-07 06:22:31.345028: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 6 Chunks of size 41472000 totalling 237.30MiB\n",
      "2022-05-07 06:22:31.345036: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 41496576 totalling 39.57MiB\n",
      "2022-05-07 06:22:31.345043: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 76435456 totalling 72.89MiB\n",
      "2022-05-07 06:22:31.345051: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 76943616 totalling 73.38MiB\n",
      "2022-05-07 06:22:31.345059: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 78943744 totalling 75.29MiB\n",
      "2022-05-07 06:22:31.345066: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 270000128 totalling 257.49MiB\n",
      "2022-05-07 06:22:31.345073: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 295731200 totalling 282.03MiB\n",
      "2022-05-07 06:22:31.345081: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 474708224 totalling 452.72MiB\n",
      "2022-05-07 06:22:31.345088: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1080000000 totalling 2.01GiB\n",
      "2022-05-07 06:22:31.345095: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 2160000000 totalling 6.03GiB\n",
      "2022-05-07 06:22:31.345102: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 9.82GiB\n",
      "2022-05-07 06:22:31.345109: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 15522398208 memory_limit_: 15522398208 available bytes: 0 curr_region_allocation_bytes_: 17179869184\n",
      "2022-05-07 06:22:31.345124: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                     15522398208\n",
      "InUse:                     10545797632\n",
      "MaxInUse:                  12116646912\n",
      "NumAllocs:                      561597\n",
      "MaxAllocSize:               2176777216\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-05-07 06:22:31.345181: W tensorflow/core/common_runtime/bfc_allocator.cc:475] ****************************____________***********************x___********______********______*****\n",
      "2022-05-07 06:22:31.346035: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_pooling_gpu.cc:153 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[50,16,30,150,150] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[50,16,30,150,150] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential_6/max_pooling3d_24/MaxPool3D/MaxPool3DGrad\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_30807]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential_6/max_pooling3d_24/MaxPool3D/MaxPool3DGrad:\nIn[0] sequential_6/batch_normalization_31/FusedBatchNormV3 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/normalization/batch_normalization.py:589)\t\nIn[1] sequential_6/max_pooling3d_24/MaxPool3D (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/pooling.py:699)\t\nIn[2] gradient_tape/sequential_6/conv3d_32/Conv3D/Conv3DBackpropInputV2:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_1959/1044347880.py\", line 5, in <module>\n>>>     model_7.execute()\n>>> \n>>>   File \"/tmp/ipykernel_1959/2649880545.py\", line 74, in execute\n>>>     self.model_arch.fit_generator(train_generator, steps_per_epoch = steps_per_epoch, epochs = self.num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2016, in fit_generator\n>>>     return self.fit(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1959/1044347880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_architecture_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n=========== Training the model ===========\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1959/2649880545.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcal_validation_steps_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         self.model_arch.fit_generator(train_generator, steps_per_epoch = steps_per_epoch, epochs = self.num_epochs, verbose=1, \n\u001b[0m\u001b[1;32m     75\u001b[0m                         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                         validation_steps = validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2016\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[50,16,30,150,150] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential_6/max_pooling3d_24/MaxPool3D/MaxPool3DGrad\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_30807]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential_6/max_pooling3d_24/MaxPool3D/MaxPool3DGrad:\nIn[0] sequential_6/batch_normalization_31/FusedBatchNormV3 (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/normalization/batch_normalization.py:589)\t\nIn[1] sequential_6/max_pooling3d_24/MaxPool3D (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/pooling.py:699)\t\nIn[2] gradient_tape/sequential_6/conv3d_32/Conv3D/Conv3DBackpropInputV2:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_1959/1044347880.py\", line 5, in <module>\n>>>     model_7.execute()\n>>> \n>>>   File \"/tmp/ipykernel_1959/2649880545.py\", line 74, in execute\n>>>     self.model_arch.fit_generator(train_generator, steps_per_epoch = steps_per_epoch, epochs = self.num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2016, in fit_generator\n>>>     return self.fit(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> "
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_7 = model_architecture(25, 50, 150)\n",
    "model_7.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_7.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From our previous runs we noticed that image with more number of epochs and increased batch size performs better. Try to execute the previous models with more number of batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-8 input summary (Image size: 120, batch size = 40)\n",
    "\n",
    "- input image size(120x120)\n",
    "- epochs: 50\n",
    "- batch size: 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_35 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 30, 120, 120, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " conv3d_36 (Conv3D)          (None, 30, 120, 120, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 30, 120, 120, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_28 (MaxPoolin  (None, 15, 60, 60, 16)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_37 (Conv3D)          (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 15, 60, 60, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_29 (MaxPoolin  (None, 7, 30, 30, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_38 (Conv3D)          (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 7, 30, 30, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_30 (MaxPoolin  (None, 3, 15, 15, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_39 (Conv3D)          (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 3, 15, 15, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_31 (MaxPoolin  (None, 1, 7, 7, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.3046 - categorical_accuracy: 0.2715Source path =  /home/datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 85s 5s/step - loss: 8.3046 - categorical_accuracy: 0.2715 - val_loss: 1.9266 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0198 - categorical_accuracy: 0.4143\n",
      "Epoch 00002: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 53s 3s/step - loss: 2.0198 - categorical_accuracy: 0.4143 - val_loss: 2.2548 - val_categorical_accuracy: 0.2333 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5563 - categorical_accuracy: 0.4959\n",
      "Epoch 00003: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.5563 - categorical_accuracy: 0.4959 - val_loss: 3.0050 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1493 - categorical_accuracy: 0.5851\n",
      "Epoch 00004: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 46s 3s/step - loss: 1.1493 - categorical_accuracy: 0.5851 - val_loss: 4.5992 - val_categorical_accuracy: 0.2667 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1100 - categorical_accuracy: 0.6198\n",
      "Epoch 00005: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 47s 3s/step - loss: 1.1100 - categorical_accuracy: 0.6198 - val_loss: 6.6943 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9081 - categorical_accuracy: 0.6678\n",
      "Epoch 00006: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.9081 - categorical_accuracy: 0.6678 - val_loss: 9.3940 - val_categorical_accuracy: 0.1833 - lr: 2.5000e-04\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7495 - categorical_accuracy: 0.7336\n",
      "Epoch 00007: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.7495 - categorical_accuracy: 0.7336 - val_loss: 8.7959 - val_categorical_accuracy: 0.2667 - lr: 2.5000e-04\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6453 - categorical_accuracy: 0.7889\n",
      "Epoch 00008: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.6453 - categorical_accuracy: 0.7889 - val_loss: 8.5309 - val_categorical_accuracy: 0.2333 - lr: 1.2500e-04\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5753 - categorical_accuracy: 0.7993\n",
      "Epoch 00009: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.5753 - categorical_accuracy: 0.7993 - val_loss: 10.6659 - val_categorical_accuracy: 0.2000 - lr: 1.2500e-04\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4835 - categorical_accuracy: 0.8304\n",
      "Epoch 00010: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4835 - categorical_accuracy: 0.8304 - val_loss: 10.6066 - val_categorical_accuracy: 0.2000 - lr: 6.2500e-05\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4601 - categorical_accuracy: 0.8408\n",
      "Epoch 00011: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4601 - categorical_accuracy: 0.8408 - val_loss: 9.7137 - val_categorical_accuracy: 0.2167 - lr: 6.2500e-05\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4500 - categorical_accuracy: 0.8304\n",
      "Epoch 00012: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.4500 - categorical_accuracy: 0.8304 - val_loss: 10.6034 - val_categorical_accuracy: 0.1833 - lr: 3.1250e-05\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4405 - categorical_accuracy: 0.8478\n",
      "Epoch 00013: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.4405 - categorical_accuracy: 0.8478 - val_loss: 9.7745 - val_categorical_accuracy: 0.2167 - lr: 3.1250e-05\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4191 - categorical_accuracy: 0.8478\n",
      "Epoch 00014: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4191 - categorical_accuracy: 0.8478 - val_loss: 8.8703 - val_categorical_accuracy: 0.2167 - lr: 1.5625e-05\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3839 - categorical_accuracy: 0.8408\n",
      "Epoch 00015: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3839 - categorical_accuracy: 0.8408 - val_loss: 7.7365 - val_categorical_accuracy: 0.2333 - lr: 1.5625e-05\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4412 - categorical_accuracy: 0.8443\n",
      "Epoch 00016: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.4412 - categorical_accuracy: 0.8443 - val_loss: 7.2433 - val_categorical_accuracy: 0.2333 - lr: 7.8125e-06\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4880 - categorical_accuracy: 0.8097\n",
      "Epoch 00017: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.4880 - categorical_accuracy: 0.8097 - val_loss: 6.6103 - val_categorical_accuracy: 0.2833 - lr: 7.8125e-06\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3378 - categorical_accuracy: 0.8893\n",
      "Epoch 00018: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3378 - categorical_accuracy: 0.8893 - val_loss: 7.3638 - val_categorical_accuracy: 0.1333 - lr: 3.9063e-06\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4161 - categorical_accuracy: 0.8443\n",
      "Epoch 00019: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4161 - categorical_accuracy: 0.8443 - val_loss: 5.6876 - val_categorical_accuracy: 0.2167 - lr: 3.9063e-06\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4115 - categorical_accuracy: 0.8374\n",
      "Epoch 00020: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.4115 - categorical_accuracy: 0.8374 - val_loss: 4.1056 - val_categorical_accuracy: 0.3167 - lr: 1.9531e-06\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3693 - categorical_accuracy: 0.8685\n",
      "Epoch 00021: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.3693 - categorical_accuracy: 0.8685 - val_loss: 4.0265 - val_categorical_accuracy: 0.3000 - lr: 1.9531e-06\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3268 - categorical_accuracy: 0.8824\n",
      "Epoch 00022: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3268 - categorical_accuracy: 0.8824 - val_loss: 3.4897 - val_categorical_accuracy: 0.3167 - lr: 9.7656e-07\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3565 - categorical_accuracy: 0.8754\n",
      "Epoch 00023: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3565 - categorical_accuracy: 0.8754 - val_loss: 2.9955 - val_categorical_accuracy: 0.3667 - lr: 9.7656e-07\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3281 - categorical_accuracy: 0.8789\n",
      "Epoch 00024: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3281 - categorical_accuracy: 0.8789 - val_loss: 2.3121 - val_categorical_accuracy: 0.4000 - lr: 4.8828e-07\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3270 - categorical_accuracy: 0.8824\n",
      "Epoch 00025: val_loss did not improve from 0.69709\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3270 - categorical_accuracy: 0.8824 - val_loss: 2.1559 - val_categorical_accuracy: 0.4333 - lr: 4.8828e-07\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4872 - categorical_accuracy: 0.8131\n",
      "Epoch 00026: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.4872 - categorical_accuracy: 0.8131 - val_loss: 1.8346 - val_categorical_accuracy: 0.5500 - lr: 2.4414e-07\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3943 - categorical_accuracy: 0.8478\n",
      "Epoch 00027: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.3943 - categorical_accuracy: 0.8478 - val_loss: 1.3136 - val_categorical_accuracy: 0.6167 - lr: 2.4414e-07\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3962 - categorical_accuracy: 0.8235\n",
      "Epoch 00028: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.3962 - categorical_accuracy: 0.8235 - val_loss: 1.2219 - val_categorical_accuracy: 0.6167 - lr: 2.4414e-07\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4200 - categorical_accuracy: 0.8512\n",
      "Epoch 00029: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.4200 - categorical_accuracy: 0.8512 - val_loss: 1.2901 - val_categorical_accuracy: 0.5500 - lr: 2.4414e-07\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3991 - categorical_accuracy: 0.8547\n",
      "Epoch 00030: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.3991 - categorical_accuracy: 0.8547 - val_loss: 0.9341 - val_categorical_accuracy: 0.6833 - lr: 2.4414e-07\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3677 - categorical_accuracy: 0.8581\n",
      "Epoch 00031: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3677 - categorical_accuracy: 0.8581 - val_loss: 1.0789 - val_categorical_accuracy: 0.6500 - lr: 2.4414e-07\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3491 - categorical_accuracy: 0.8616\n",
      "Epoch 00032: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.3491 - categorical_accuracy: 0.8616 - val_loss: 0.8676 - val_categorical_accuracy: 0.7000 - lr: 2.4414e-07\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3789 - categorical_accuracy: 0.8720\n",
      "Epoch 00033: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3789 - categorical_accuracy: 0.8720 - val_loss: 0.7394 - val_categorical_accuracy: 0.7500 - lr: 2.4414e-07\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3459 - categorical_accuracy: 0.8893\n",
      "Epoch 00034: val_loss did not improve from 0.69709\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.3459 - categorical_accuracy: 0.8893 - val_loss: 0.8937 - val_categorical_accuracy: 0.7167 - lr: 2.4414e-07\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3882 - categorical_accuracy: 0.8512\n",
      "Epoch 00035: val_loss improved from 0.69709 to 0.69647, saving model to model_init_2022-05-0704_47_36.064098/model-00035-0.38824-0.85121-0.69647-0.76667.h5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.3882 - categorical_accuracy: 0.8512 - val_loss: 0.6965 - val_categorical_accuracy: 0.7667 - lr: 2.4414e-07\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3732 - categorical_accuracy: 0.8685\n",
      "Epoch 00036: val_loss did not improve from 0.69647\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3732 - categorical_accuracy: 0.8685 - val_loss: 0.7165 - val_categorical_accuracy: 0.7333 - lr: 2.4414e-07\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4345 - categorical_accuracy: 0.8270\n",
      "Epoch 00037: val_loss improved from 0.69647 to 0.67684, saving model to model_init_2022-05-0704_47_36.064098/model-00037-0.43445-0.82699-0.67684-0.73333.h5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4345 - categorical_accuracy: 0.8270 - val_loss: 0.6768 - val_categorical_accuracy: 0.7333 - lr: 2.4414e-07\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4401 - categorical_accuracy: 0.8235\n",
      "Epoch 00038: val_loss did not improve from 0.67684\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4401 - categorical_accuracy: 0.8235 - val_loss: 0.7373 - val_categorical_accuracy: 0.8000 - lr: 2.4414e-07\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3483 - categorical_accuracy: 0.8858\n",
      "Epoch 00039: val_loss improved from 0.67684 to 0.62277, saving model to model_init_2022-05-0704_47_36.064098/model-00039-0.34826-0.88581-0.62277-0.76667.h5\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3483 - categorical_accuracy: 0.8858 - val_loss: 0.6228 - val_categorical_accuracy: 0.7667 - lr: 2.4414e-07\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3977 - categorical_accuracy: 0.8581\n",
      "Epoch 00040: val_loss did not improve from 0.62277\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3977 - categorical_accuracy: 0.8581 - val_loss: 0.6484 - val_categorical_accuracy: 0.8000 - lr: 2.4414e-07\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4062 - categorical_accuracy: 0.8270\n",
      "Epoch 00041: val_loss did not improve from 0.62277\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4062 - categorical_accuracy: 0.8270 - val_loss: 0.6873 - val_categorical_accuracy: 0.8000 - lr: 2.4414e-07\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3744 - categorical_accuracy: 0.8547\n",
      "Epoch 00042: val_loss improved from 0.62277 to 0.58949, saving model to model_init_2022-05-0704_47_36.064098/model-00042-0.37443-0.85467-0.58949-0.81667.h5\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3744 - categorical_accuracy: 0.8547 - val_loss: 0.5895 - val_categorical_accuracy: 0.8167 - lr: 1.2207e-07\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4097 - categorical_accuracy: 0.8339\n",
      "Epoch 00043: val_loss did not improve from 0.58949\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.4097 - categorical_accuracy: 0.8339 - val_loss: 0.6291 - val_categorical_accuracy: 0.8667 - lr: 1.2207e-07\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3755 - categorical_accuracy: 0.8685\n",
      "Epoch 00044: val_loss improved from 0.58949 to 0.50502, saving model to model_init_2022-05-0704_47_36.064098/model-00044-0.37554-0.86851-0.50502-0.81667.h5\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.3755 - categorical_accuracy: 0.8685 - val_loss: 0.5050 - val_categorical_accuracy: 0.8167 - lr: 1.2207e-07\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3253 - categorical_accuracy: 0.9031\n",
      "Epoch 00045: val_loss did not improve from 0.50502\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3253 - categorical_accuracy: 0.9031 - val_loss: 0.7872 - val_categorical_accuracy: 0.8000 - lr: 1.2207e-07\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4974 - categorical_accuracy: 0.8097\n",
      "Epoch 00046: val_loss did not improve from 0.50502\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4974 - categorical_accuracy: 0.8097 - val_loss: 0.5660 - val_categorical_accuracy: 0.8500 - lr: 1.2207e-07\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3763 - categorical_accuracy: 0.8616\n",
      "Epoch 00047: val_loss did not improve from 0.50502\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.3763 - categorical_accuracy: 0.8616 - val_loss: 0.6105 - val_categorical_accuracy: 0.7833 - lr: 6.1035e-08\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4190 - categorical_accuracy: 0.8512\n",
      "Epoch 00048: val_loss did not improve from 0.50502\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.4190 - categorical_accuracy: 0.8512 - val_loss: 0.6394 - val_categorical_accuracy: 0.7833 - lr: 6.1035e-08\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3937 - categorical_accuracy: 0.8443\n",
      "Epoch 00049: val_loss did not improve from 0.50502\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.3937 - categorical_accuracy: 0.8443 - val_loss: 0.7169 - val_categorical_accuracy: 0.7667 - lr: 3.0518e-08\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4420 - categorical_accuracy: 0.8616\n",
      "Epoch 00050: val_loss improved from 0.50502 to 0.43362, saving model to model_init_2022-05-0704_47_36.064098/model-00050-0.44202-0.86159-0.43362-0.88333.h5\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.4420 - categorical_accuracy: 0.8616 - val_loss: 0.4336 - val_categorical_accuracy: 0.8833 - lr: 3.0518e-08\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_8 = model_architecture(50, 40, 120)\n",
    "model_8.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_8.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-8 output summary\n",
    "- Training accuracy:  86.16\n",
    "- Validation accuracy: 88.33\n",
    "\n",
    "We can observe that training the model with more number of batch sizes brings down the validation loss and improved the validation accuracy. But the training loss went down. Later we try to run with more number of epocs to see the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-9 input summary (Image size: 150, batch size = 40)\n",
    "\n",
    "- input image size(150x150)\n",
    "- epochs: 50\n",
    "- batch size: 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_40 (Conv3D)          (None, 30, 150, 150, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 30, 150, 150, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 30, 150, 150, 8)   0         \n",
      "                                                                 \n",
      " conv3d_41 (Conv3D)          (None, 30, 150, 150, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 30, 150, 150, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_32 (MaxPoolin  (None, 15, 75, 75, 16)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_42 (Conv3D)          (None, 15, 75, 75, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 15, 75, 75, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_33 (MaxPoolin  (None, 7, 37, 37, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_43 (Conv3D)          (None, 7, 37, 37, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 7, 37, 37, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_34 (MaxPoolin  (None, 3, 18, 18, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_44 (Conv3D)          (None, 3, 18, 18, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 3, 18, 18, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_35 (MaxPoolin  (None, 1, 9, 9, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 10368)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1000)              10369000  \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,963,365\n",
      "Trainable params: 10,962,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 9.8093 - categorical_accuracy: 0.3047 Source path =  /home/datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 97s 6s/step - loss: 9.8093 - categorical_accuracy: 0.3047 - val_loss: 2.9602 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.3266 - categorical_accuracy: 0.4476\n",
      "Epoch 00002: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 58s 4s/step - loss: 2.3266 - categorical_accuracy: 0.4476 - val_loss: 8.6892 - val_categorical_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8180 - categorical_accuracy: 0.4687\n",
      "Epoch 00003: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 54s 3s/step - loss: 1.8180 - categorical_accuracy: 0.4687 - val_loss: 5.7458 - val_categorical_accuracy: 0.2333 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6286 - categorical_accuracy: 0.5480\n",
      "Epoch 00004: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 49s 3s/step - loss: 1.6286 - categorical_accuracy: 0.5480 - val_loss: 5.8454 - val_categorical_accuracy: 0.1000 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1102 - categorical_accuracy: 0.6390\n",
      "Epoch 00005: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 47s 3s/step - loss: 1.1102 - categorical_accuracy: 0.6390 - val_loss: 7.6410 - val_categorical_accuracy: 0.1000 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0039 - categorical_accuracy: 0.7024\n",
      "Epoch 00006: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.0039 - categorical_accuracy: 0.7024 - val_loss: 6.5185 - val_categorical_accuracy: 0.1667 - lr: 2.5000e-04\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8671 - categorical_accuracy: 0.7024\n",
      "Epoch 00007: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.8671 - categorical_accuracy: 0.7024 - val_loss: 7.5134 - val_categorical_accuracy: 0.1000 - lr: 2.5000e-04\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6875 - categorical_accuracy: 0.7232\n",
      "Epoch 00008: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.6875 - categorical_accuracy: 0.7232 - val_loss: 7.2189 - val_categorical_accuracy: 0.1000 - lr: 1.2500e-04\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7642 - categorical_accuracy: 0.7509\n",
      "Epoch 00009: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.7642 - categorical_accuracy: 0.7509 - val_loss: 7.1293 - val_categorical_accuracy: 0.2167 - lr: 1.2500e-04\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6184 - categorical_accuracy: 0.7716\n",
      "Epoch 00010: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.6184 - categorical_accuracy: 0.7716 - val_loss: 6.2862 - val_categorical_accuracy: 0.1000 - lr: 6.2500e-05\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5091 - categorical_accuracy: 0.8235\n",
      "Epoch 00011: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5091 - categorical_accuracy: 0.8235 - val_loss: 6.5605 - val_categorical_accuracy: 0.1833 - lr: 6.2500e-05\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6124 - categorical_accuracy: 0.7682\n",
      "Epoch 00012: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.6124 - categorical_accuracy: 0.7682 - val_loss: 7.3198 - val_categorical_accuracy: 0.1333 - lr: 3.1250e-05\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5532 - categorical_accuracy: 0.7958\n",
      "Epoch 00013: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.5532 - categorical_accuracy: 0.7958 - val_loss: 6.7307 - val_categorical_accuracy: 0.1500 - lr: 3.1250e-05\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6298 - categorical_accuracy: 0.7751\n",
      "Epoch 00014: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.6298 - categorical_accuracy: 0.7751 - val_loss: 6.0825 - val_categorical_accuracy: 0.2167 - lr: 1.5625e-05\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5162 - categorical_accuracy: 0.7958\n",
      "Epoch 00015: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.5162 - categorical_accuracy: 0.7958 - val_loss: 7.9725 - val_categorical_accuracy: 0.1167 - lr: 1.5625e-05\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5932 - categorical_accuracy: 0.7751\n",
      "Epoch 00016: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5932 - categorical_accuracy: 0.7751 - val_loss: 5.9238 - val_categorical_accuracy: 0.1667 - lr: 7.8125e-06\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5638 - categorical_accuracy: 0.7993\n",
      "Epoch 00017: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.5638 - categorical_accuracy: 0.7993 - val_loss: 7.0392 - val_categorical_accuracy: 0.1333 - lr: 7.8125e-06\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5119 - categorical_accuracy: 0.8097\n",
      "Epoch 00021: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4930 - categorical_accuracy: 0.8062 - val_loss: 4.7407 - val_categorical_accuracy: 0.2833 - lr: 1.9531e-06\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5747 - categorical_accuracy: 0.7785\n",
      "Epoch 00022: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.5747 - categorical_accuracy: 0.7785 - val_loss: 4.4300 - val_categorical_accuracy: 0.2500 - lr: 9.7656e-07\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5299 - categorical_accuracy: 0.8201\n",
      "Epoch 00023: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5299 - categorical_accuracy: 0.8201 - val_loss: 4.6088 - val_categorical_accuracy: 0.2167 - lr: 9.7656e-07\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4831 - categorical_accuracy: 0.8201\n",
      "Epoch 00024: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4831 - categorical_accuracy: 0.8201 - val_loss: 3.9069 - val_categorical_accuracy: 0.2833 - lr: 4.8828e-07\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5040 - categorical_accuracy: 0.8028\n",
      "Epoch 00025: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5040 - categorical_accuracy: 0.8028 - val_loss: 3.8806 - val_categorical_accuracy: 0.2333 - lr: 4.8828e-07\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5842 - categorical_accuracy: 0.7924\n",
      "Epoch 00026: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.5842 - categorical_accuracy: 0.7924 - val_loss: 2.6627 - val_categorical_accuracy: 0.3667 - lr: 2.4414e-07\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5198 - categorical_accuracy: 0.7924\n",
      "Epoch 00027: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5198 - categorical_accuracy: 0.7924 - val_loss: 2.6262 - val_categorical_accuracy: 0.3667 - lr: 2.4414e-07\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4416 - categorical_accuracy: 0.8443\n",
      "Epoch 00028: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.4416 - categorical_accuracy: 0.8443 - val_loss: 2.1449 - val_categorical_accuracy: 0.4333 - lr: 2.4414e-07\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5933 - categorical_accuracy: 0.8131\n",
      "Epoch 00029: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.5933 - categorical_accuracy: 0.8131 - val_loss: 2.0472 - val_categorical_accuracy: 0.4500 - lr: 2.4414e-07\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5664 - categorical_accuracy: 0.7993\n",
      "Epoch 00030: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.5664 - categorical_accuracy: 0.7993 - val_loss: 1.5053 - val_categorical_accuracy: 0.5833 - lr: 2.4414e-07\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5174 - categorical_accuracy: 0.7958\n",
      "Epoch 00031: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.5174 - categorical_accuracy: 0.7958 - val_loss: 1.7545 - val_categorical_accuracy: 0.4000 - lr: 2.4414e-07\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6545 - categorical_accuracy: 0.7578\n",
      "Epoch 00032: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.6545 - categorical_accuracy: 0.7578 - val_loss: 1.3159 - val_categorical_accuracy: 0.5833 - lr: 2.4414e-07\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5580 - categorical_accuracy: 0.7958\n",
      "Epoch 00033: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5580 - categorical_accuracy: 0.7958 - val_loss: 1.3669 - val_categorical_accuracy: 0.5667 - lr: 2.4414e-07\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5372 - categorical_accuracy: 0.7751\n",
      "Epoch 00034: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5372 - categorical_accuracy: 0.7751 - val_loss: 1.0636 - val_categorical_accuracy: 0.6000 - lr: 2.4414e-07\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4966 - categorical_accuracy: 0.8028\n",
      "Epoch 00035: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.4966 - categorical_accuracy: 0.8028 - val_loss: 0.9893 - val_categorical_accuracy: 0.6167 - lr: 2.4414e-07\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4898 - categorical_accuracy: 0.8166\n",
      "Epoch 00036: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4898 - categorical_accuracy: 0.8166 - val_loss: 0.8846 - val_categorical_accuracy: 0.7000 - lr: 2.4414e-07\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5283 - categorical_accuracy: 0.8028\n",
      "Epoch 00037: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 46s 3s/step - loss: 0.5283 - categorical_accuracy: 0.8028 - val_loss: 0.8241 - val_categorical_accuracy: 0.7000 - lr: 2.4414e-07\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5806 - categorical_accuracy: 0.8131\n",
      "Epoch 00038: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5806 - categorical_accuracy: 0.8131 - val_loss: 0.6665 - val_categorical_accuracy: 0.7667 - lr: 2.4414e-07\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4942 - categorical_accuracy: 0.8097\n",
      "Epoch 00039: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.4942 - categorical_accuracy: 0.8097 - val_loss: 0.9777 - val_categorical_accuracy: 0.6333 - lr: 2.4414e-07\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6416 - categorical_accuracy: 0.7578\n",
      "Epoch 00040: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.6416 - categorical_accuracy: 0.7578 - val_loss: 0.7651 - val_categorical_accuracy: 0.7000 - lr: 2.4414e-07\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5396 - categorical_accuracy: 0.7958\n",
      "Epoch 00041: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.5396 - categorical_accuracy: 0.7958 - val_loss: 0.8534 - val_categorical_accuracy: 0.7000 - lr: 1.2207e-07\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5487 - categorical_accuracy: 0.8062\n",
      "Epoch 00042: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5487 - categorical_accuracy: 0.8062 - val_loss: 0.7266 - val_categorical_accuracy: 0.7000 - lr: 1.2207e-07\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5641 - categorical_accuracy: 0.7993\n",
      "Epoch 00043: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 43s 3s/step - loss: 0.5641 - categorical_accuracy: 0.7993 - val_loss: 0.7404 - val_categorical_accuracy: 0.7000 - lr: 6.1035e-08\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5169 - categorical_accuracy: 0.8131\n",
      "Epoch 00044: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "17/17 [==============================] - 47s 3s/step - loss: 0.5169 - categorical_accuracy: 0.8131 - val_loss: 0.8032 - val_categorical_accuracy: 0.6333 - lr: 6.1035e-08\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5948 - categorical_accuracy: 0.7855\n",
      "Epoch 00045: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5948 - categorical_accuracy: 0.7855 - val_loss: 0.7841 - val_categorical_accuracy: 0.7500 - lr: 3.0518e-08\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5253 - categorical_accuracy: 0.7993\n",
      "Epoch 00046: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5253 - categorical_accuracy: 0.7993 - val_loss: 0.6963 - val_categorical_accuracy: 0.7000 - lr: 3.0518e-08\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5355 - categorical_accuracy: 0.8166\n",
      "Epoch 00047: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 45s 3s/step - loss: 0.5355 - categorical_accuracy: 0.8166 - val_loss: 0.8337 - val_categorical_accuracy: 0.6667 - lr: 1.5259e-08\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5558 - categorical_accuracy: 0.7855\n",
      "Epoch 00048: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5558 - categorical_accuracy: 0.7855 - val_loss: 0.6662 - val_categorical_accuracy: 0.8167 - lr: 1.5259e-08\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5370 - categorical_accuracy: 0.8097\n",
      "Epoch 00049: val_loss did not improve from 0.43362\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.5370 - categorical_accuracy: 0.8097 - val_loss: 0.6730 - val_categorical_accuracy: 0.7500 - lr: 1.5259e-08\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4783 - categorical_accuracy: 0.8131\n",
      "Epoch 00050: val_loss did not improve from 0.43362\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 0.4783 - categorical_accuracy: 0.8131 - val_loss: 1.0599 - val_categorical_accuracy: 0.6167 - lr: 1.5259e-08\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_9 = model_architecture(50, 40, 150)\n",
    "model_9.define_architecture_1();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_9.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-9 output summary\n",
    "- Training accuracy: 81.31\n",
    "- Validation accuracy: 61.67  \n",
    "\n",
    "We can observe that training the model with Image size (150x150) with higher number of batches brings down the accuracy and performed poorly compared to image size of 120x120. \n",
    "\n",
    "##### Going further, model with different layer of archtectures can be trained with 120x120 image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different architecture(2)\n",
    "\n",
    "#### Model-10 (Image-size 120x120)\n",
    "- num_epoch: 25\n",
    "- batch_size: 40\n",
    "- image_size: 120x120\n",
    "\n",
    "##### Model training with less number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-07 14:40:59.130379: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-05-07 14:40:59.130451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14803 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1c:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 30, 120, 120, 8)  32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 30, 120, 120, 16)  3472      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 120, 120, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 15, 60, 60, 16)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 15, 60, 60, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 7, 30, 30, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 7, 30, 30, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 3, 15, 15, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 3, 15, 15, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 7, 7, 128)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 7, 7, 128)      0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,867,365\n",
      "Trainable params: 6,866,869\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-07 14:41:06.102871: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 10.7104 - categorical_accuracy: 0.2504Source path =  /home/datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17877, saving model to model_init_2022-05-0714_40_18.671989/model-00001-10.71037-0.25038-2.17877-0.21000.h5\n",
      "17/17 [==============================] - 92s 5s/step - loss: 10.7104 - categorical_accuracy: 0.2504 - val_loss: 2.1788 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.5576 - categorical_accuracy: 0.4092\n",
      "Epoch 00002: val_loss did not improve from 2.17877\n",
      "17/17 [==============================] - 52s 3s/step - loss: 2.5576 - categorical_accuracy: 0.4092 - val_loss: 4.2571 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9002 - categorical_accuracy: 0.4060\n",
      "Epoch 00003: val_loss improved from 2.17877 to 1.91464, saving model to model_init_2022-05-0714_40_18.671989/model-00003-1.90021-0.40599-1.91464-0.31667.h5\n",
      "17/17 [==============================] - 51s 3s/step - loss: 1.9002 - categorical_accuracy: 0.4060 - val_loss: 1.9146 - val_categorical_accuracy: 0.3167 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8506 - categorical_accuracy: 0.3808\n",
      "Epoch 00004: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.8506 - categorical_accuracy: 0.3808 - val_loss: 4.5616 - val_categorical_accuracy: 0.2167 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3635 - categorical_accuracy: 0.4856\n",
      "Epoch 00005: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 45s 3s/step - loss: 1.3635 - categorical_accuracy: 0.4856 - val_loss: 7.1832 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1657 - categorical_accuracy: 0.5467\n",
      "Epoch 00006: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 40s 2s/step - loss: 1.1657 - categorical_accuracy: 0.5467 - val_loss: 5.9298 - val_categorical_accuracy: 0.2833 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1126 - categorical_accuracy: 0.5848\n",
      "Epoch 00007: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 40s 3s/step - loss: 1.1126 - categorical_accuracy: 0.5848 - val_loss: 9.3899 - val_categorical_accuracy: 0.1500 - lr: 5.0000e-04\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9308 - categorical_accuracy: 0.6644\n",
      "Epoch 00008: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 40s 3s/step - loss: 0.9308 - categorical_accuracy: 0.6644 - val_loss: 10.1493 - val_categorical_accuracy: 0.1667 - lr: 2.5000e-04\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8011 - categorical_accuracy: 0.6782\n",
      "Epoch 00009: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.8011 - categorical_accuracy: 0.6782 - val_loss: 10.7617 - val_categorical_accuracy: 0.2333 - lr: 2.5000e-04\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8261 - categorical_accuracy: 0.6713\n",
      "Epoch 00010: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.8261 - categorical_accuracy: 0.6713 - val_loss: 10.0332 - val_categorical_accuracy: 0.2167 - lr: 1.2500e-04\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7230 - categorical_accuracy: 0.7266\n",
      "Epoch 00011: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.7230 - categorical_accuracy: 0.7266 - val_loss: 10.4753 - val_categorical_accuracy: 0.2167 - lr: 1.2500e-04\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5866 - categorical_accuracy: 0.7820\n",
      "Epoch 00012: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.5866 - categorical_accuracy: 0.7820 - val_loss: 11.1513 - val_categorical_accuracy: 0.2333 - lr: 6.2500e-05\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6870 - categorical_accuracy: 0.6990\n",
      "Epoch 00013: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.6870 - categorical_accuracy: 0.6990 - val_loss: 10.9897 - val_categorical_accuracy: 0.2000 - lr: 6.2500e-05\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7716 - categorical_accuracy: 0.6851\n",
      "Epoch 00014: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.7716 - categorical_accuracy: 0.6851 - val_loss: 9.8983 - val_categorical_accuracy: 0.2500 - lr: 3.1250e-05\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6243 - categorical_accuracy: 0.7716\n",
      "Epoch 00015: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.6243 - categorical_accuracy: 0.7716 - val_loss: 9.6774 - val_categorical_accuracy: 0.2667 - lr: 3.1250e-05\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6604 - categorical_accuracy: 0.7474\n",
      "Epoch 00016: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 0.6604 - categorical_accuracy: 0.7474 - val_loss: 8.5311 - val_categorical_accuracy: 0.3333 - lr: 1.5625e-05\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6645 - categorical_accuracy: 0.7439\n",
      "Epoch 00017: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.6645 - categorical_accuracy: 0.7439 - val_loss: 9.6391 - val_categorical_accuracy: 0.2167 - lr: 1.5625e-05\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6689 - categorical_accuracy: 0.7336\n",
      "Epoch 00018: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.6689 - categorical_accuracy: 0.7336 - val_loss: 8.7626 - val_categorical_accuracy: 0.2500 - lr: 7.8125e-06\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5965 - categorical_accuracy: 0.7958\n",
      "Epoch 00019: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "17/17 [==============================] - 38s 2s/step - loss: 0.5965 - categorical_accuracy: 0.7958 - val_loss: 9.0350 - val_categorical_accuracy: 0.2667 - lr: 7.8125e-06\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5704 - categorical_accuracy: 0.7474\n",
      "Epoch 00020: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.5704 - categorical_accuracy: 0.7474 - val_loss: 9.0714 - val_categorical_accuracy: 0.1833 - lr: 3.9063e-06\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6132 - categorical_accuracy: 0.7682\n",
      "Epoch 00021: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.6132 - categorical_accuracy: 0.7682 - val_loss: 6.6104 - val_categorical_accuracy: 0.3167 - lr: 3.9063e-06\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5821 - categorical_accuracy: 0.7889\n",
      "Epoch 00022: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 42s 3s/step - loss: 0.5821 - categorical_accuracy: 0.7889 - val_loss: 6.9922 - val_categorical_accuracy: 0.2833 - lr: 1.9531e-06\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6641 - categorical_accuracy: 0.7612\n",
      "Epoch 00023: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.6641 - categorical_accuracy: 0.7612 - val_loss: 7.3864 - val_categorical_accuracy: 0.2333 - lr: 1.9531e-06\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6418 - categorical_accuracy: 0.7405\n",
      "Epoch 00024: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 39s 2s/step - loss: 0.6418 - categorical_accuracy: 0.7405 - val_loss: 7.3662 - val_categorical_accuracy: 0.2167 - lr: 9.7656e-07\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6080 - categorical_accuracy: 0.7543\n",
      "Epoch 00025: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "17/17 [==============================] - 40s 3s/step - loss: 0.6080 - categorical_accuracy: 0.7543 - val_loss: 5.8828 - val_categorical_accuracy: 0.2667 - lr: 9.7656e-07\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_10 = model_architecture(25, 40, 120)\n",
    "model_10.define_architecture_2();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_10.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-10 output summary\n",
    "- Training accuracy: 75.43\n",
    "- Validation accuracy: 26.67\n",
    "\n",
    "Deep learning models are evaluated should be based on the loss. Though the training loss is very low but the validation loss is quite high compared to model 1.\n",
    "Model architecture 1 seems to be better compared to architecture 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different architecture(3)\n",
    "\n",
    "#### Model-10 (Image-size 120x120)\n",
    "- num_epoch: 25\n",
    "- batch_size: 40\n",
    "- image_size: 120x120\n",
    "\n",
    "##### Model training with less number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_13 (Conv3D)          (None, 30, 120, 120, 16)  1312      \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 30, 120, 120, 16)  6928      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 30, 120, 120, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 15, 60, 60, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 15, 60, 60, 32)    4128      \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 15, 60, 60, 32)    8224      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 15, 60, 60, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 7, 30, 30, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 7, 30, 30, 64)     16448     \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 7, 30, 30, 64)     32832     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 7, 30, 30, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_10 (MaxPoolin  (None, 3, 15, 15, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 3, 15, 15, 128)    65664     \n",
      "                                                                 \n",
      " conv3d_20 (Conv3D)          (None, 3, 15, 15, 128)    131200    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 3, 15, 15, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_11 (MaxPoolin  (None, 1, 7, 7, 128)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1, 7, 7, 128)      0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1000)              6273000   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,043,701\n",
      "Trainable params: 7,043,221\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "=========== Training the model ===========\n",
      "\n",
      "\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 40\n",
      "Epoch 1/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 8.2165 - categorical_accuracy: 0.2655Source path =  /home/datasets/Project_data/val ; batch size = 40\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 85s 5s/step - loss: 8.2165 - categorical_accuracy: 0.2655 - val_loss: 2.9361 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.3670 - categorical_accuracy: 0.3453\n",
      "Epoch 00002: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 52s 3s/step - loss: 2.3670 - categorical_accuracy: 0.3453 - val_loss: 5.6156 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1019 - categorical_accuracy: 0.4033\n",
      "Epoch 00003: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.1019 - categorical_accuracy: 0.4033 - val_loss: 6.5445 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7548 - categorical_accuracy: 0.3963\n",
      "Epoch 00004: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 43s 3s/step - loss: 1.7548 - categorical_accuracy: 0.3963 - val_loss: 4.2510 - val_categorical_accuracy: 0.1833 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6871 - categorical_accuracy: 0.4121\n",
      "Epoch 00005: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "17/17 [==============================] - 44s 3s/step - loss: 1.6871 - categorical_accuracy: 0.4121 - val_loss: 4.9678 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4715 - categorical_accuracy: 0.4775\n",
      "Epoch 00006: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 38s 2s/step - loss: 1.4715 - categorical_accuracy: 0.4775 - val_loss: 4.7836 - val_categorical_accuracy: 0.1333 - lr: 2.5000e-04\n",
      "Epoch 7/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1600 - categorical_accuracy: 0.5087\n",
      "Epoch 00007: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.1600 - categorical_accuracy: 0.5087 - val_loss: 5.5086 - val_categorical_accuracy: 0.2167 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2795 - categorical_accuracy: 0.4775\n",
      "Epoch 00008: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 1.2795 - categorical_accuracy: 0.4775 - val_loss: 5.5437 - val_categorical_accuracy: 0.1333 - lr: 1.2500e-04\n",
      "Epoch 9/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3541 - categorical_accuracy: 0.4844\n",
      "Epoch 00009: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "17/17 [==============================] - 40s 2s/step - loss: 1.3541 - categorical_accuracy: 0.4844 - val_loss: 6.0460 - val_categorical_accuracy: 0.2167 - lr: 1.2500e-04\n",
      "Epoch 10/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1287 - categorical_accuracy: 0.5571\n",
      "Epoch 00010: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.1287 - categorical_accuracy: 0.5571 - val_loss: 5.7497 - val_categorical_accuracy: 0.2333 - lr: 6.2500e-05\n",
      "Epoch 11/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2013 - categorical_accuracy: 0.5052\n",
      "Epoch 00011: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 1.2013 - categorical_accuracy: 0.5052 - val_loss: 5.8516 - val_categorical_accuracy: 0.1667 - lr: 6.2500e-05\n",
      "Epoch 12/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0561 - categorical_accuracy: 0.5363\n",
      "Epoch 00012: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 1.0561 - categorical_accuracy: 0.5363 - val_loss: 6.2841 - val_categorical_accuracy: 0.1833 - lr: 3.1250e-05\n",
      "Epoch 13/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1518 - categorical_accuracy: 0.4983\n",
      "Epoch 00013: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.1518 - categorical_accuracy: 0.4983 - val_loss: 6.0677 - val_categorical_accuracy: 0.2333 - lr: 3.1250e-05\n",
      "Epoch 14/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1519 - categorical_accuracy: 0.5225\n",
      "Epoch 00014: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 40s 2s/step - loss: 1.1519 - categorical_accuracy: 0.5225 - val_loss: 5.8757 - val_categorical_accuracy: 0.2167 - lr: 1.5625e-05\n",
      "Epoch 15/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2003 - categorical_accuracy: 0.5121\n",
      "Epoch 00015: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 1.2003 - categorical_accuracy: 0.5121 - val_loss: 6.5452 - val_categorical_accuracy: 0.2167 - lr: 1.5625e-05\n",
      "Epoch 16/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1153 - categorical_accuracy: 0.5675\n",
      "Epoch 00016: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 1.1153 - categorical_accuracy: 0.5675 - val_loss: 6.3726 - val_categorical_accuracy: 0.1667 - lr: 7.8125e-06\n",
      "Epoch 17/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1012 - categorical_accuracy: 0.5813\n",
      "Epoch 00017: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.1012 - categorical_accuracy: 0.5813 - val_loss: 6.4411 - val_categorical_accuracy: 0.2333 - lr: 7.8125e-06\n",
      "Epoch 18/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1115 - categorical_accuracy: 0.5709\n",
      "Epoch 00018: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 39s 2s/step - loss: 1.1115 - categorical_accuracy: 0.5709 - val_loss: 7.1880 - val_categorical_accuracy: 0.1667 - lr: 3.9063e-06\n",
      "Epoch 19/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1224 - categorical_accuracy: 0.5190\n",
      "Epoch 00019: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.1224 - categorical_accuracy: 0.5190 - val_loss: 6.3009 - val_categorical_accuracy: 0.1833 - lr: 3.9063e-06\n",
      "Epoch 20/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0613 - categorical_accuracy: 0.5709\n",
      "Epoch 00020: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 41s 3s/step - loss: 1.0613 - categorical_accuracy: 0.5709 - val_loss: 6.0806 - val_categorical_accuracy: 0.2833 - lr: 1.9531e-06\n",
      "Epoch 21/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0773 - categorical_accuracy: 0.5744\n",
      "Epoch 00021: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "17/17 [==============================] - 38s 2s/step - loss: 1.0773 - categorical_accuracy: 0.5744 - val_loss: 6.8754 - val_categorical_accuracy: 0.1500 - lr: 1.9531e-06\n",
      "Epoch 22/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0643 - categorical_accuracy: 0.5675\n",
      "Epoch 00022: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 40s 3s/step - loss: 1.0643 - categorical_accuracy: 0.5675 - val_loss: 6.3041 - val_categorical_accuracy: 0.2333 - lr: 9.7656e-07\n",
      "Epoch 23/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0655 - categorical_accuracy: 0.5882\n",
      "Epoch 00023: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "17/17 [==============================] - 40s 2s/step - loss: 1.0655 - categorical_accuracy: 0.5882 - val_loss: 6.6057 - val_categorical_accuracy: 0.2333 - lr: 9.7656e-07\n",
      "Epoch 24/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1072 - categorical_accuracy: 0.5363\n",
      "Epoch 00024: val_loss did not improve from 1.91464\n",
      "17/17 [==============================] - 42s 3s/step - loss: 1.1072 - categorical_accuracy: 0.5363 - val_loss: 7.1901 - val_categorical_accuracy: 0.1500 - lr: 4.8828e-07\n",
      "Epoch 25/25\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0289 - categorical_accuracy: 0.5536\n",
      "Epoch 00025: val_loss did not improve from 1.91464\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "17/17 [==============================] - 39s 2s/step - loss: 1.0289 - categorical_accuracy: 0.5536 - val_loss: 5.8777 - val_categorical_accuracy: 0.2667 - lr: 4.8828e-07\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture with num_epoch, batch_size, input image size\n",
    "model_11 = model_architecture(25, 40, 120)\n",
    "model_11.define_architecture_3();\n",
    "print('\\n\\n=========== Training the model ===========\\n\\n')\n",
    "model_11.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-11 output summary\n",
    "- Training accuracy: 55.36\n",
    "- Validation accuracy: 26.67\n",
    "\n",
    "Compared to architecture-1, architecture-3 seems to perform very bad with same number of epochs=25, batch_size=40 and image size of 120. Validation loss seems to be high compared to model-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
